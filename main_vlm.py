import os
import sys
import cv2
import mujoco
import matplotlib.pyplot as plt 
import time

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(ROOT_DIR, 'graspnet-baseline', 'models'))
sys.path.append(os.path.join(ROOT_DIR, 'graspnet-baseline', 'dataset'))
sys.path.append(os.path.join(ROOT_DIR, 'graspnet-baseline', 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'manipulator_grasp'))

from manipulator_grasp.env.ur5_grasp_env import UR5GraspEnv

from vlm_process import segment_image
# from grasp_process import run_grasp_inference, execute_grasp
from grasp_process_optimized import run_grasp_inference, execute_grasp
import spatialmath as sm
import numpy as np


# å…¨å±€å˜é‡
global color_img, depth_img, env
color_img = None
depth_img = None
env = None


#è·å–å½©è‰²å’Œæ·±åº¦å›¾åƒæ•°æ®
def get_image(env, camera_name=None):
    global color_img, depth_img
     # ä»ç¯å¢ƒæ¸²æŸ“è·å–å›¾åƒæ•°æ®
    imgs = env.render(camera_name=camera_name)

    # æå–å½©è‰²å’Œæ·±åº¦å›¾åƒæ•°æ®
    color_img = imgs['img']   # è¿™æ˜¯RGBæ ¼å¼çš„å›¾åƒæ•°æ®
    depth_img = imgs['depth'] # è¿™æ˜¯æ·±åº¦æ•°æ®

    # å°†RGBå›¾åƒè½¬æ¢ä¸ºOpenCVå¸¸ç”¨çš„BGRæ ¼å¼
    color_img = cv2.cvtColor(color_img, cv2.COLOR_RGB2BGR)

    return color_img, depth_img

#æ„é€ å›è°ƒå‡½æ•°ï¼Œä¸æ–­è°ƒç”¨
def callback(color_frame, depth_frame):
    global color_img, depth_img
    scaling_factor_x = 1
    scaling_factor_y = 1

    color_img = cv2.resize(
        color_frame, None,
        fx=scaling_factor_x,
        fy=scaling_factor_y,
        interpolation=cv2.INTER_AREA
    )
    depth_img = cv2.resize(
        depth_frame, None,
        fx=scaling_factor_x,
        fy=scaling_factor_y,
        interpolation=cv2.INTER_NEAREST
    )

    if color_img is not None and depth_img is not None:
        test_grasp()


def test_grasp():
    global color_img, depth_img, env

    if color_img is None or depth_img is None:
        print("[WARNING] Waiting for image data...")
        return

    # --- åŠ¨æ€è·å–ç›¸æœºå‚æ•° ---
    cam_name = "cam"
    cam_id = mujoco.mj_name2id(env.mj_model, mujoco.mjtObj.mjOBJ_CAMERA, cam_name)
    
    # è·å–ç›¸æœºå¤–éƒ¨å‚æ•° (World-to-Camera Transform)
    # data.cam_xpos: ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½ç½®
    # data.cam_xmat: ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„æ—‹è½¬çŸ©é˜µ (3x3)
    t_wc = env.mj_data.cam_xpos[cam_id]
    # è·å–æ—‹è½¬çŸ©é˜µ (MuJoCo é»˜è®¤ä¸º 3x3)
    R_mj = env.mj_data.cam_xmat[cam_id].reshape(3, 3)
    
    # [æ ¸å¿ƒä¿®å¤] ç›´æ¥é€šè¿‡åˆ—å‘é‡å˜æ¢å°† MuJoCo åæ ‡ç³»è½¬ä¸º CV æ ‡å‡†åæ ‡ç³»
    # CV_X = MuJoCo_X (ç¬¬ä¸€åˆ—)
    # CV_Y = -MuJoCo_Y (ç¬¬äºŒåˆ—å–å)
    # CV_Z = -MuJoCo_Z (ç¬¬ä¸‰åˆ—å–å)
    R_cv = np.column_stack([
        R_mj[:, 0], 
        -R_mj[:, 1], 
        -R_mj[:, 2]
    ])
    
    T_wc = sm.SE3.Rt(R_cv, t_wc)
    
    # è·å–ç›¸æœºå†…éƒ¨å‚æ•° (fovy)
    # model.cam_fovy: å‚ç›´è§†åœºè§’ (è§’åº¦åˆ¶)ï¼Œè½¬ä¸ºå¼§åº¦
    fovy_deg = env.mj_model.cam_fovy[cam_id]
    fovy_rad = np.deg2rad(fovy_deg)

    # --- Debug: å¯¹æ¯”åŠ¨æ€æå–å€¼ä¸åŸå§‹ç¡¬ç¼–ç å€¼ ---
    print(f"\n[DEBUG] Camera '{cam_name}' (ID: {cam_id})")
    print(f"  Position (MuJoCo): {t_wc}")
    print(f"  FOVY (Deg): {fovy_deg:.2f}")
    
    # è®¡ç®—åŸå§‹ç¡¬ç¼–ç çš„ T_wc ç”¨äºå¯¹æ¯”
    n_wc_orig = np.array([0.0, -1.0, 0.0])
    o_wc_orig = np.array([-1.0, 0.0, -0.5])
    t_wc_orig = np.array([0.85, 0.8, 1.6])
    T_wc_orig = sm.SE3.Trans(t_wc_orig) * sm.SE3(sm.SO3.TwoVectors(x=n_wc_orig, y=o_wc_orig))
    
    print(f"\n  --- åŸå§‹ç¡¬ç¼–ç  T_wc.R ---")
    print(T_wc_orig.R)
    print(f"\n  --- åŠ¨æ€æå– R_cv ---")
    print(R_cv)
    print(f"\n  --- å·®å¼‚ (åº”è¯¥æ¥è¿‘0å¦‚æœæ­£ç¡®) ---")
    print(np.abs(T_wc_orig.R - R_cv).max())

    # å›¾åƒå¤„ç†éƒ¨åˆ†
    masks = segment_image(color_img)  

    # ä¼ å…¥åŠ¨æ€æå–çš„ç›¸æœºå‚æ•°
    gg = run_grasp_inference(color_img, depth_img, masks, T_wc=T_wc, fovy=fovy_rad)

    execute_grasp(env, gg, T_wc=T_wc)



if __name__ == '__main__':
    
    env = UR5GraspEnv()
    env.reset()
    
    # ç›¸æœºé…ç½®
    CAMERA_TABLE = "cam"        # è§‚å¯Ÿæ¡Œé¢
    CAMERA_SHELF = "cam_shelf"  # è§‚å¯Ÿè´§æ¶
    current_mode = "single"     # single æˆ– fusion
    current_camera = CAMERA_TABLE
    
    # å¯¼å…¥èåˆå‡½æ•°
    from grasp_process_optimized import run_grasp_inference_fused
    
    while True:

        for i in range(500):
            env.step()

        # é€‰æ‹©æ¨¡å¼
        print(f"\nğŸ“· å½“å‰æ¨¡å¼: {current_mode.upper()}")
        print("   è¾“å…¥ '1' å•ç›¸æœºæ¨¡å¼ - æ¡Œé¢ç›¸æœº (cam)")
        print("   è¾“å…¥ '2' å•ç›¸æœºæ¨¡å¼ - è´§æ¶ç›¸æœº (cam_shelf)")
        print("   è¾“å…¥ '3' èåˆæ¨¡å¼ - åŒç›¸æœºç‚¹äº‘èåˆ ğŸ”¥")
        print("   ç›´æ¥æŒ‰å›è½¦ç»§ç»­...")
        
        choice = input("é€‰æ‹©: ").strip()
        
        if choice == '1':
            current_mode = "single"
            current_camera = CAMERA_TABLE
            print(f"âœ… å•ç›¸æœºæ¨¡å¼: {current_camera}")
        elif choice == '2':
            current_mode = "single"
            current_camera = CAMERA_SHELF
            print(f"âœ… å•ç›¸æœºæ¨¡å¼: {current_camera}")
        elif choice == '3':
            current_mode = "fusion"
            print("âœ… èåˆæ¨¡å¼: å°†ä½¿ç”¨åŒç›¸æœºç‚¹äº‘èåˆ")
        
        if current_mode == "single":
            # å•ç›¸æœºæ¨¡å¼
            color_img, depth_img = get_image(env, camera_name=current_camera)
            callback(color_img, depth_img)
        
        elif current_mode == "fusion":
            # èåˆæ¨¡å¼
            print("\n[FUSION] é‡‡é›†åŒç›¸æœºå›¾åƒ...")
            
            # ä»ä¸¤ä¸ªç›¸æœºé‡‡é›†
            imgs_cam = env.render(camera_name=CAMERA_TABLE)
            imgs_shelf = env.render(camera_name=CAMERA_SHELF)
            
            # è·å–ç›¸æœºå‚æ•°
            def get_cam_params(env, cam_name):
                cam_id = mujoco.mj_name2id(env.mj_model, mujoco.mjtObj.mjOBJ_CAMERA, cam_name)
                t_wc = env.mj_data.cam_xpos[cam_id].copy()
                R_mj = env.mj_data.cam_xmat[cam_id].reshape(3, 3)
                R_cv = np.column_stack([R_mj[:, 0], -R_mj[:, 1], -R_mj[:, 2]])
                T_wc = sm.SE3.Rt(R_cv, t_wc)
                fovy = np.deg2rad(env.mj_model.cam_fovy[cam_id])
                return T_wc, fovy
            
            T_wc_table, fovy_table = get_cam_params(env, CAMERA_TABLE)
            T_wc_shelf, fovy_shelf = get_cam_params(env, CAMERA_SHELF)
            
            # VLM åˆ†å‰² - å¯¹ä¸¤ä¸ªç›¸æœºå›¾åƒéƒ½è¿›è¡Œç›®æ ‡åˆ†å‰²
            color_img_table = cv2.cvtColor(imgs_cam['img'], cv2.COLOR_RGB2BGR)
            color_img_shelf = cv2.cvtColor(imgs_shelf['img'], cv2.COLOR_RGB2BGR)
            
            # è·å–ç”¨æˆ·æŒ‡ä»¤ï¼ˆåªè¯¢é—®ä¸€æ¬¡ï¼‰
            print("\nğŸ“ [FUSION] è¯·é€šè¿‡æ–‡å­—æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
            user_command = input("è¯·è¾“å…¥: ").strip()
            
            print("\n[FUSION] VLM åˆ†å‰²æ¡Œé¢ç›¸æœºå›¾åƒ...")
            masks_table = segment_image(color_img_table, command_text=user_command)
            
            print("\n[FUSION] VLM åˆ†å‰²è´§æ¶ç›¸æœºå›¾åƒ...")
            masks_shelf = segment_image(color_img_shelf, command_text=user_command)

            
            # å‡†å¤‡èåˆæ•°æ® - ä¸¤ä¸ªç›¸æœºéƒ½ä½¿ç”¨å„è‡ªçš„åˆ†å‰²ç»“æœ
            camera_data_list = [
                {
                    'color': imgs_cam['img'],
                    'depth': imgs_cam['depth'],
                    'mask': masks_table,
                    'T_wc': T_wc_table,
                    'fovy': fovy_table
                },
                {
                    'color': imgs_shelf['img'],
                    'depth': imgs_shelf['depth'],
                    'mask': masks_shelf,  # ä½¿ç”¨ VLM åˆ†å‰²çš„æ©ç 
                    'T_wc': T_wc_shelf,
                    'fovy': fovy_shelf
                }
            ]

            
            # èåˆæ¨ç†
            gg = run_grasp_inference_fused(camera_data_list, T_wc_table, fovy_table)
            
            if gg is not None:
                execute_grasp(env, gg, T_wc=T_wc_table)


    env.close()


    