import os
import sys
import cv2
import mujoco
import matplotlib.pyplot as plt 
import time

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(ROOT_DIR, 'graspnet-baseline', 'models'))
sys.path.append(os.path.join(ROOT_DIR, 'graspnet-baseline', 'dataset'))
sys.path.append(os.path.join(ROOT_DIR, 'graspnet-baseline', 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'manipulator_grasp'))

from manipulator_grasp.env.ur5_grasp_env import UR5GraspEnv

from vlm_process import segment_image, parse_instruction, detect_place_position, pixel_to_world
# from grasp_process import run_grasp_inference, execute_grasp
from grasp_process_optimized import run_grasp_inference, execute_grasp
import spatialmath as sm
import numpy as np


# å…¨å±€å˜é‡
global color_img, depth_img, env
color_img = None
depth_img = None
env = None


#è·å–å½©è‰²å’Œæ·±åº¦å›¾åƒæ•°æ®
def get_image(env, camera_name=None):
    global color_img, depth_img
     # ä»ç¯å¢ƒæ¸²æŸ“è·å–å›¾åƒæ•°æ®
    imgs = env.render(camera_name=camera_name)

    # æå–å½©è‰²å’Œæ·±åº¦å›¾åƒæ•°æ®
    color_img = imgs['img']   # è¿™æ˜¯RGBæ ¼å¼çš„å›¾åƒæ•°æ®
    depth_img = imgs['depth'] # è¿™æ˜¯æ·±åº¦æ•°æ®

    # å°†RGBå›¾åƒè½¬æ¢ä¸ºOpenCVå¸¸ç”¨çš„BGRæ ¼å¼
    color_img = cv2.cvtColor(color_img, cv2.COLOR_RGB2BGR)

    return color_img, depth_img

#æ„é€ å›è°ƒå‡½æ•°ï¼Œä¸æ–­è°ƒç”¨
def callback(color_frame, depth_frame):
    global color_img, depth_img
    scaling_factor_x = 1
    scaling_factor_y = 1

    color_img = cv2.resize(
        color_frame, None,
        fx=scaling_factor_x,
        fy=scaling_factor_y,
        interpolation=cv2.INTER_AREA
    )
    depth_img = cv2.resize(
        depth_frame, None,
        fx=scaling_factor_x,
        fy=scaling_factor_y,
        interpolation=cv2.INTER_NEAREST
    )

    if color_img is not None and depth_img is not None:
        test_grasp()


def test_grasp():
    global color_img, depth_img, env

    if color_img is None or depth_img is None:
        print("[WARNING] Waiting for image data...")
        return

    # --- åŠ¨æ€è·å–ç›¸æœºå‚æ•° ---
    cam_name = "cam"
    cam_id = mujoco.mj_name2id(env.mj_model, mujoco.mjtObj.mjOBJ_CAMERA, cam_name)
    
    # è·å–ç›¸æœºå¤–éƒ¨å‚æ•° (World-to-Camera Transform)
    # data.cam_xpos: ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½ç½®
    # data.cam_xmat: ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„æ—‹è½¬çŸ©é˜µ (3x3)
    t_wc = env.mj_data.cam_xpos[cam_id]
    # è·å–æ—‹è½¬çŸ©é˜µ (MuJoCo é»˜è®¤ä¸º 3x3)
    R_mj = env.mj_data.cam_xmat[cam_id].reshape(3, 3)
    
    # [æ ¸å¿ƒä¿®å¤] ç›´æ¥é€šè¿‡åˆ—å‘é‡å˜æ¢å°† MuJoCo åæ ‡ç³»è½¬ä¸º CV æ ‡å‡†åæ ‡ç³»
    # CV_X = MuJoCo_X (ç¬¬ä¸€åˆ—)
    # CV_Y = -MuJoCo_Y (ç¬¬äºŒåˆ—å–å)
    # CV_Z = -MuJoCo_Z (ç¬¬ä¸‰åˆ—å–å)
    R_cv = np.column_stack([
        R_mj[:, 0], 
        -R_mj[:, 1], 
        -R_mj[:, 2]
    ])
    
    T_wc = sm.SE3.Rt(R_cv, t_wc)
    
    # è·å–ç›¸æœºå†…éƒ¨å‚æ•° (fovy)
    # model.cam_fovy: å‚ç›´è§†åœºè§’ (è§’åº¦åˆ¶)ï¼Œè½¬ä¸ºå¼§åº¦
    fovy_deg = env.mj_model.cam_fovy[cam_id]
    fovy_rad = np.deg2rad(fovy_deg)

    # --- Debug: å¯¹æ¯”åŠ¨æ€æå–å€¼ä¸åŸå§‹ç¡¬ç¼–ç å€¼ ---
    print(f"\n[DEBUG] Camera '{cam_name}' (ID: {cam_id})")
    print(f"  Position (MuJoCo): {t_wc}")
    print(f"  FOVY (Deg): {fovy_deg:.2f}")
    
    # è®¡ç®—åŸå§‹ç¡¬ç¼–ç çš„ T_wc ç”¨äºå¯¹æ¯”
    n_wc_orig = np.array([0.0, -1.0, 0.0])
    o_wc_orig = np.array([-1.0, 0.0, -0.5])
    t_wc_orig = np.array([0.85, 0.8, 1.6])
    T_wc_orig = sm.SE3.Trans(t_wc_orig) * sm.SE3(sm.SO3.TwoVectors(x=n_wc_orig, y=o_wc_orig))
    
    print(f"\n  --- åŸå§‹ç¡¬ç¼–ç  T_wc.R ---")
    print(T_wc_orig.R)
    print(f"\n  --- åŠ¨æ€æå– R_cv ---")
    print(R_cv)
    print(f"\n  --- å·®å¼‚ (åº”è¯¥æ¥è¿‘0å¦‚æœæ­£ç¡®) ---")
    print(np.abs(T_wc_orig.R - R_cv).max())

    # å›¾åƒå¤„ç†éƒ¨åˆ†
    masks = segment_image(color_img)  

    # ä¼ å…¥åŠ¨æ€æå–çš„ç›¸æœºå‚æ•°
    gg = run_grasp_inference(color_img, depth_img, masks, T_wc=T_wc, fovy=fovy_rad)

    execute_grasp(env, gg, T_wc=T_wc)



if __name__ == '__main__':
    
    env = UR5GraspEnv()
    env.reset()
    
    # ç›¸æœºé…ç½®
    CAMERA_TABLE = "cam"        # è§‚å¯Ÿæ¡Œé¢
    CAMERA_SHELF = "cam_shelf"  # è§‚å¯Ÿè´§æ¶
    CAMERA_GLOBAL_1 = "cam_global_1"  # å…¨å±€ç›¸æœº1
    CAMERA_GLOBAL_2 = "cam_global_2"  # å…¨å±€ç›¸æœº2
    current_mode = "single"     # single, fusion, æˆ– smart
    current_camera = CAMERA_TABLE

    # å¯¼å…¥èåˆå‡½æ•°
    from grasp_process_optimized import run_grasp_inference_fused

    # è¾…åŠ©å‡½æ•°ï¼šè·å–ç›¸æœºå‚æ•°
    def get_cam_params(env, cam_name):
        cam_id = mujoco.mj_name2id(env.mj_model, mujoco.mjtObj.mjOBJ_CAMERA, cam_name)
        t_wc = env.mj_data.cam_xpos[cam_id].copy()
        R_mj = env.mj_data.cam_xmat[cam_id].reshape(3, 3)
        R_cv = np.column_stack([R_mj[:, 0], -R_mj[:, 1], -R_mj[:, 2]])
        T_wc = sm.SE3.Rt(R_cv, t_wc)
        fovy = np.deg2rad(env.mj_model.cam_fovy[cam_id])
        return T_wc, fovy

    while True:

        for i in range(500):
            env.step()

        # é€‰æ‹©æ¨¡å¼
        print(f"\nğŸ“· å½“å‰æ¨¡å¼: {current_mode.upper()}")
        print("   è¾“å…¥ '1' å•ç›¸æœºæ¨¡å¼ - æ¡Œé¢ç›¸æœº (cam)")
        print("   è¾“å…¥ '2' å•ç›¸æœºæ¨¡å¼ - è´§æ¶ç›¸æœº (cam_shelf)")
        print("   è¾“å…¥ '3' èåˆæ¨¡å¼ - åŒç›¸æœºç‚¹äº‘èåˆ")
        print("   è¾“å…¥ '4' æ™ºèƒ½æ”¾ç½®æ¨¡å¼ - è‡ªç„¶è¯­è¨€æŒ‡å®šæ”¾ç½®ä½ç½® ğŸ”¥")
        print("   ç›´æ¥æŒ‰å›è½¦ç»§ç»­...")

        choice = input("é€‰æ‹©: ").strip()

        if choice == '1':
            current_mode = "single"
            current_camera = CAMERA_TABLE
            print(f"âœ… å•ç›¸æœºæ¨¡å¼: {current_camera}")
        elif choice == '2':
            current_mode = "single"
            current_camera = CAMERA_SHELF
            print(f"âœ… å•ç›¸æœºæ¨¡å¼: {current_camera}")
        elif choice == '3':
            current_mode = "fusion"
            print("âœ… èåˆæ¨¡å¼: å°†ä½¿ç”¨åŒç›¸æœºç‚¹äº‘èåˆ")
        elif choice == '4':
            current_mode = "smart"
            print("âœ… æ™ºèƒ½æ”¾ç½®æ¨¡å¼: æ”¯æŒè‡ªç„¶è¯­è¨€æŒ‡å®šæ”¾ç½®ä½ç½®")

        if current_mode == "single":
            # å•ç›¸æœºæ¨¡å¼
            color_img, depth_img = get_image(env, camera_name=current_camera)
            callback(color_img, depth_img)

        elif current_mode == "smart":
            # æ™ºèƒ½æ”¾ç½®æ¨¡å¼ - æ”¯æŒè‡ªç„¶è¯­è¨€æŒ‡å®šæ”¾ç½®ä½ç½®
            print("\n" + "="*60)
            print("ğŸ¤– æ™ºèƒ½æ”¾ç½®æ¨¡å¼")
            print("="*60)
            print("è¯·è¾“å…¥å®Œæ•´çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼š")
            print("  - æŠŠåŸ¹å…»çš¿æ”¾ç½®åˆ°æ˜¾å¾®é•œçš„å³è¾¹")
            print("  - æŠŠè¯•ç®¡ç§»åˆ°æ¡Œå­å·¦ä¸Šè§’")
            print("  - æŠ“å–çƒ§æ¯æ”¾åˆ°çº¢è‰²åŒºåŸŸ")
            print("="*60)

            user_input = input("\nè¯·è¾“å…¥æŒ‡ä»¤: ").strip()
            if not user_input:
                print("âš ï¸ æœªè¾“å…¥æŒ‡ä»¤ï¼Œè·³è¿‡")
                continue

            # 1. è§£æç”¨æˆ·æŒ‡ä»¤
            print("\n[Step 1] è§£æç”¨æˆ·æŒ‡ä»¤...")
            instruction = parse_instruction(user_input)
            grasp_target = instruction.get("grasp_target", user_input)
            place_description = instruction.get("place_description", "")
            has_place = instruction.get("has_place_instruction", False)

            print(f"  æŠ“å–ç›®æ ‡: {grasp_target}")
            print(f"  æ”¾ç½®ä½ç½®: {place_description if place_description else '(æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤ä½ç½®)'}")

            # 2. è·å–æ¡Œé¢ç›¸æœºå›¾åƒè¿›è¡ŒæŠ“å–è¯†åˆ«
            print("\n[Step 2] è·å–æ¡Œé¢ç›¸æœºå›¾åƒ...")
            color_img, depth_img = get_image(env, camera_name=CAMERA_TABLE)
            T_wc_table, fovy_table = get_cam_params(env, CAMERA_TABLE)

            # 3. VLMåˆ†å‰²ç›®æ ‡ç‰©ä½“
            print("\n[Step 3] VLMè¯†åˆ«æŠ“å–ç›®æ ‡...")
            masks = segment_image(color_img, command_text=grasp_target)

            # 4. æŠ“å–æ¨ç†
            print("\n[Step 4] æŠ“å–å§¿æ€æ¨ç†...")
            gg = run_grasp_inference(color_img, depth_img, masks, T_wc=T_wc_table, fovy=fovy_table)

            if gg is None:
                print("âŒ æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„æŠ“å–å§¿æ€")
                continue

            # 5. ç¡®å®šæ”¾ç½®ä½ç½®
            target_pos = None
            if has_place and place_description:
                print("\n[Step 5] è¯†åˆ«æ”¾ç½®ä½ç½®...")

                # è·å–å¤šä¸ªå…¨å±€ç›¸æœºå›¾åƒ
                print("  è·å–å¤šè§†è§’å…¨å±€ç›¸æœºå›¾åƒ...")

                # ä¸»ç›¸æœº (ç”¨äºåæ ‡è®¡ç®—)
                imgs_global_2 = env.render(camera_name=CAMERA_GLOBAL_2)
                color_global = cv2.cvtColor(imgs_global_2['img'], cv2.COLOR_RGB2BGR)
                depth_global = imgs_global_2['depth']
                T_wc_global, fovy_global = get_cam_params(env, CAMERA_GLOBAL_2)

                # é¢å¤–ç›¸æœº (ç”¨äºè¾…åŠ©è¯†åˆ«)
                extra_images = []
                try:
                    imgs_global_1 = env.render(camera_name=CAMERA_GLOBAL_1)
                    color_global_1 = cv2.cvtColor(imgs_global_1['img'], cv2.COLOR_RGB2BGR)
                    extra_images.append(color_global_1)
                    cv2.imwrite("debug_global_view_1.jpg", color_global_1)
                except:
                    print("  cam_global_1 ä¸å¯ç”¨")

                # å¦‚æœæœ‰ cam_global_3
                try:
                    imgs_global_3 = env.render(camera_name="cam_global_3")
                    color_global_3 = cv2.cvtColor(imgs_global_3['img'], cv2.COLOR_RGB2BGR)
                    extra_images.append(color_global_3)
                    cv2.imwrite("debug_global_view_3.jpg", color_global_3)
                except:
                    pass  # cam_global_3 å¯èƒ½ä¸å­˜åœ¨

                print(f"  ä½¿ç”¨ {1 + len(extra_images)} ä¸ªç›¸æœºè§†è§’")

                # ä¿å­˜ä¸»ç›¸æœºå›¾åƒç”¨äºè°ƒè¯•
                cv2.imwrite("debug_global_view.jpg", color_global)
                print("  å…¨å±€è§†å›¾å·²ä¿å­˜: debug_global_view.jpg")

                # VLMè¯†åˆ«æ”¾ç½®ä½ç½® (ä¼ å…¥å¤šç›¸æœºå›¾åƒ)
                place_result = detect_place_position(place_description, color_global, extra_images=extra_images)

                if place_result and "place_point" in place_result:
                    pixel_x, pixel_y = place_result["place_point"]
                    print(f"  VLMè¯†åˆ«çš„æ”¾ç½®ä½ç½®: åƒç´ åæ ‡ ({pixel_x}, {pixel_y})")
                    print(f"  ç½®ä¿¡åº¦: {place_result.get('confidence', 'N/A')}")
                    print(f"  åŸå› : {place_result.get('reason', 'N/A')}")

                    # åœ¨å…¨å±€å›¾åƒä¸Šæ ‡æ³¨æ”¾ç½®ä½ç½®å’Œå‚è€ƒç‰©ä½“
                    debug_img = color_global.copy()

                    # æ ‡æ³¨å‚è€ƒç‰©ä½“ä½ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
                    if "reference_position" in place_result:
                        ref_x, ref_y = place_result["reference_position"]
                        cv2.circle(debug_img, (int(ref_x), int(ref_y)), 12, (255, 0, 0), -1)  # è“è‰²åœ†ç‚¹
                        cv2.circle(debug_img, (int(ref_x), int(ref_y)), 16, (255, 255, 0), 2)  # é’è‰²è¾¹æ¡†
                        cv2.putText(debug_img, "Reference", (int(ref_x)+20, int(ref_y)-10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                        print(f"  å‚è€ƒç‰©ä½“ä½ç½®: ({ref_x}, {ref_y})")

                    # æ ‡æ³¨æ”¾ç½®ä½ç½®
                    cv2.circle(debug_img, (pixel_x, pixel_y), 15, (0, 0, 255), -1)  # çº¢è‰²åœ†ç‚¹
                    cv2.circle(debug_img, (pixel_x, pixel_y), 20, (0, 255, 0), 3)   # ç»¿è‰²è¾¹æ¡†
                    cv2.putText(debug_img, "Place Here", (pixel_x+25, pixel_y),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                    cv2.imwrite("debug_place_position.jpg", debug_img)
                    print("  æ ‡æ³¨å›¾åƒå·²ä¿å­˜: debug_place_position.jpg")
                    print("    - è“è‰²åœ†ç‚¹: å‚è€ƒç‰©ä½“ä½ç½®")
                    print("    - çº¢è‰²åœ†ç‚¹: æ”¾ç½®ä½ç½®")

                    # åƒç´ åæ ‡è½¬ä¸–ç•Œåæ ‡
                    world_pos = pixel_to_world(
                        pixel_x, pixel_y, depth_global,
                        T_wc_global, fovy_global, color_global.shape
                    )

                    # è®¾ç½®æ”¾ç½®é«˜åº¦ï¼ˆæ¡Œé¢é«˜åº¦çº¦0.74mï¼ŒåŠ ä¸Šä¸€ç‚¹ä½™é‡ï¼‰
                    target_pos = [world_pos[0], world_pos[1], 0.76]
                    print(f"  åŸå§‹ä¸–ç•Œåæ ‡: ({target_pos[0]:.3f}, {target_pos[1]:.3f}, {target_pos[2]:.3f})")

                    # æ£€æŸ¥å¹¶è°ƒæ•´åˆ°æœºæ¢°è‡‚å·¥ä½œç©ºé—´å†…
                    # UR5eæœºæ¢°è‡‚åŸºåº§åœ¨åŸç‚¹é™„è¿‘ï¼Œå·¥ä½œåŠå¾„çº¦0.85m
                    # å®‰å…¨å·¥ä½œèŒƒå›´ï¼šx: [0.1, 1.0], y: [0.1, 0.9]
                    WORKSPACE_X_MIN, WORKSPACE_X_MAX = 0.1, 1.0
                    WORKSPACE_Y_MIN, WORKSPACE_Y_MAX = 0.1, 0.9

                    original_pos = target_pos.copy()
                    target_pos[0] = max(WORKSPACE_X_MIN, min(WORKSPACE_X_MAX, target_pos[0]))
                    target_pos[1] = max(WORKSPACE_Y_MIN, min(WORKSPACE_Y_MAX, target_pos[1]))

                    if original_pos[0] != target_pos[0] or original_pos[1] != target_pos[1]:
                        print(f"  âš ï¸ åŸå§‹ä½ç½®è¶…å‡ºå·¥ä½œç©ºé—´ï¼Œå·²è°ƒæ•´!")
                        print(f"  è°ƒæ•´ååæ ‡: ({target_pos[0]:.3f}, {target_pos[1]:.3f}, {target_pos[2]:.3f})")
                    else:
                        print(f"  ä¸–ç•Œåæ ‡: ({target_pos[0]:.3f}, {target_pos[1]:.3f}, {target_pos[2]:.3f})")
                else:
                    print("  âš ï¸ æœªèƒ½è¯†åˆ«æ”¾ç½®ä½ç½®ï¼Œä½¿ç”¨é»˜è®¤ä½ç½®")

            # 6. æ‰§è¡ŒæŠ“å–å’Œæ”¾ç½®
            print("\n[Step 6] æ‰§è¡ŒæŠ“å–å’Œæ”¾ç½®...")
            execute_grasp(env, gg, T_wc=T_wc_table, target_pos=target_pos)

            print("\nâœ… ä»»åŠ¡å®Œæˆ!")

        elif current_mode == "fusion":
            # èåˆæ¨¡å¼
            print("\n[FUSION] é‡‡é›†åŒç›¸æœºå›¾åƒ...")
            
            # ä»ä¸¤ä¸ªç›¸æœºé‡‡é›†
            imgs_cam = env.render(camera_name=CAMERA_TABLE)
            imgs_shelf = env.render(camera_name=CAMERA_SHELF)
            
            # è·å–ç›¸æœºå‚æ•°
            def get_cam_params(env, cam_name):
                cam_id = mujoco.mj_name2id(env.mj_model, mujoco.mjtObj.mjOBJ_CAMERA, cam_name)
                t_wc = env.mj_data.cam_xpos[cam_id].copy()
                R_mj = env.mj_data.cam_xmat[cam_id].reshape(3, 3)
                R_cv = np.column_stack([R_mj[:, 0], -R_mj[:, 1], -R_mj[:, 2]])
                T_wc = sm.SE3.Rt(R_cv, t_wc)
                fovy = np.deg2rad(env.mj_model.cam_fovy[cam_id])
                return T_wc, fovy
            
            T_wc_table, fovy_table = get_cam_params(env, CAMERA_TABLE)
            T_wc_shelf, fovy_shelf = get_cam_params(env, CAMERA_SHELF)
            
            # VLM åˆ†å‰² - å¯¹ä¸¤ä¸ªç›¸æœºå›¾åƒéƒ½è¿›è¡Œç›®æ ‡åˆ†å‰²
            color_img_table = cv2.cvtColor(imgs_cam['img'], cv2.COLOR_RGB2BGR)
            color_img_shelf = cv2.cvtColor(imgs_shelf['img'], cv2.COLOR_RGB2BGR)
            
            # è·å–ç”¨æˆ·æŒ‡ä»¤ï¼ˆåªè¯¢é—®ä¸€æ¬¡ï¼‰
            print("\nğŸ“ [FUSION] è¯·é€šè¿‡æ–‡å­—æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
            user_command = input("è¯·è¾“å…¥: ").strip()
            
            print("\n[FUSION] VLM åˆ†å‰²æ¡Œé¢ç›¸æœºå›¾åƒ...")
            masks_table = segment_image(color_img_table, command_text=user_command)
            
            print("\n[FUSION] VLM åˆ†å‰²è´§æ¶ç›¸æœºå›¾åƒ...")
            masks_shelf = segment_image(color_img_shelf, command_text=user_command)

            
            # å‡†å¤‡èåˆæ•°æ® - ä¸¤ä¸ªç›¸æœºéƒ½ä½¿ç”¨å„è‡ªçš„åˆ†å‰²ç»“æœ
            camera_data_list = [
                {
                    'color': imgs_cam['img'],
                    'depth': imgs_cam['depth'],
                    'mask': masks_table,
                    'T_wc': T_wc_table,
                    'fovy': fovy_table
                },
                {
                    'color': imgs_shelf['img'],
                    'depth': imgs_shelf['depth'],
                    'mask': masks_shelf,  # ä½¿ç”¨ VLM åˆ†å‰²çš„æ©ç 
                    'T_wc': T_wc_shelf,
                    'fovy': fovy_shelf
                }
            ]

            
            # èåˆæ¨ç†
            gg = run_grasp_inference_fused(camera_data_list, T_wc_table, fovy_table)
            
            if gg is not None:
                execute_grasp(env, gg, T_wc=T_wc_table)


    env.close()


    