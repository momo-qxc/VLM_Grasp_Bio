# VLM_Grasp_Bio 自然语言抓取功能技术文档

## 目录

1. [项目概述](#1-项目概述)
2. [系统架构](#2-系统架构)
3. [核心功能实现](#3-核心功能实现)
4. [遇到的问题与解决方案](#4-遇到的问题与解决方案)
5. [使用示例](#5-使用示例)
6. [待改进项](#6-待改进项)

---

## 1. 项目概述

### 1.1 项目背景

VLM_Grasp_Bio 是一个基于视觉语言模型（VLM）的机器人抓取系统，运行在 MuJoCo 仿真环境中，使用 UR5e 机械臂执行抓取任务。

**原有功能：**
- 单相机模式：使用桌面相机或货架相机进行目标识别和抓取
- 融合模式：双相机点云融合，提高抓取精度
- 固定放置位置：抓取后放置到预设的固定位置

**新增功能：**
- **智能放置模式**：支持自然语言指令，用户可以通过自然语言描述抓取目标和放置位置
- 例如："把培养皿放置到显微镜的右边红色区域"

### 1.2 技术栈

| 组件 | 技术选型 |
|------|----------|
| 仿真环境 | MuJoCo |
| 机械臂 | UR5e |
| VLM模型 | 阿里云 Qwen-VL-Max |
| 分割模型 | SAM (Segment Anything Model) |
| 抓取网络 | GraspNet |
| 坐标变换 | spatialmath |
| 点云处理 | Open3D |

---

## 2. 系统架构

### 2.1 整体流程

```
用户输入自然语言指令
        ↓
┌───────────────────────────────────────┐
│  Step 1: 指令解析 (parse_instruction) │
│  分离抓取目标和放置位置描述            │
└───────────────────────────────────────┘
        ↓
┌───────────────────────────────────────┐
│  Step 2: 获取桌面相机图像              │
│  color_img, depth_img                 │
└───────────────────────────────────────┘
        ↓
┌───────────────────────────────────────┐
│  Step 3: VLM目标识别 + SAM分割         │
│  segment_image() → 生成目标掩码        │
└───────────────────────────────────────┘
        ↓
┌───────────────────────────────────────┐
│  Step 4: GraspNet抓取姿态推理          │
│  run_grasp_inference() → 最佳抓取姿态  │
└───────────────────────────────────────┘
        ↓
┌───────────────────────────────────────┐
│  Step 5: 放置位置识别                  │
│  detect_place_position() → 像素坐标   │
│  pixel_to_world() → 世界坐标          │
└───────────────────────────────────────┘
        ↓
┌───────────────────────────────────────┐
│  Step 6: 执行抓取和放置                │
│  execute_grasp(env, gg, target_pos)   │
└───────────────────────────────────────┘
```

### 2.2 核心模块

| 模块 | 文件 | 功能 |
|------|------|------|
| 指令解析 | `vlm_process.py` | 解析自然语言，分离抓取目标和放置描述 |
| 放置识别 | `vlm_process.py` | 识别放置位置的像素坐标 |
| 坐标转换 | `vlm_process.py` | 像素坐标转世界坐标 |
| 抓取推理 | `grasp_process_optimized.py` | GraspNet抓取姿态预测 |
| 抓取执行 | `grasp_process_optimized.py` | 机械臂运动规划和执行 |
| 主控制 | `main_vlm.py` | 整体流程控制 |

---

## 3. 核心功能实现

### 3.1 自然语言指令解析 (parse_instruction)

**功能描述：**
将用户的自然语言指令分解为两部分：
1. `grasp_target`: 要抓取的物体
2. `place_description`: 放置位置的描述

**实现位置：** `vlm_process.py:38-118`

**核心代码：**

```python
def parse_instruction(user_input, image_input=None):
    """
    解析用户的自然语言指令，分离抓取目标和放置位置描述。

    输入: "把培养皿放置到显微镜的右边红色区域"
    输出: {
        "grasp_target": "培养皿",
        "place_description": "显微镜的右边红色区域",
        "has_place_instruction": True
    }
    """
    client = OpenAI(
        api_key='sk-xxx',
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        http_client=httpx.Client(trust_env=False)
    )

    system_prompt = textwrap.dedent("""\
    你是一个机器人指令解析系统。请分析用户的自然语言指令，提取以下信息：

    1. 要抓取的物体名称（grasp_target）
    2. 放置位置的描述（place_description）- 如果用户没有指定放置位置，则为空字符串

    【示例】
    输入: "把培养皿放置到显微镜的右边"
    输出: {"grasp_target": "培养皿", "place_description": "显微镜的右边", "has_place_instruction": true}

    输入: "抓取红色的试管"
    输出: {"grasp_target": "红色的试管", "place_description": "", "has_place_instruction": false}
    """)
    # ... 调用VLM并解析JSON响应
```

**设计要点：**
1. 使用 Qwen-VL-Max 模型进行语义理解
2. 通过精心设计的 System Prompt 引导模型输出结构化 JSON
3. 支持可选的图像输入，帮助理解上下文
4. 包含容错处理：如果解析失败，将整个输入作为抓取目标

---

### 3.2 智能放置位置识别 (detect_place_position)

**功能描述：**
根据放置位置的自然语言描述，在全局相机图像中识别具体的像素坐标。

**实现位置：** `vlm_process.py:121-413`

#### 3.2.1 两阶段识别策略

为了提高识别准确率，我们采用了**两阶段识别策略**：

**第一阶段：识别参考物体**
```python
# 解析放置描述，提取参考物体和方向
patterns = [
    (r'(.+?)的(左边|右边|上面|下面|前面|后面|旁边)', lambda m: (m.group(1), m.group(2))),
    (r'(左边|右边|上面|下面|前面|后面)的(.+)', lambda m: (m.group(2), m.group(1))),
]

for pattern, extractor in patterns:
    match_result = re.search(pattern, place_description)
    if match_result:
        reference_object, direction = extractor(match_result)
        break
```

例如，对于 "显微镜的右边"：
- `reference_object` = "显微镜"
- `direction` = "右边"

**第二阶段：计算放置位置**
```python
# 找到参考物体后，根据方向计算偏移
if direction == "左边":
    place_x, place_y = ref_x - pixel_offset, ref_y
elif direction == "右边":
    place_x, place_y = ref_x + pixel_offset, ref_y
elif direction in ["上面", "前面"]:
    place_x, place_y = ref_x, ref_y - pixel_offset
elif direction in ["下面", "后面"]:
    place_x, place_y = ref_x, ref_y + pixel_offset
```

#### 3.2.2 多相机融合识别

**问题背景：** 单个相机视角可能无法完整看到所有物体（如显微镜可能被遮挡）

**解决方案：** 支持多相机图像输入，拼接成全景图进行识别

```python
def detect_place_position(place_description, global_image, depth_image=None, extra_images=None):
    """
    支持多相机图像输入，提供更全面的场景理解。
    """
    if extra_images and len(extra_images) > 0:
        print(f"[放置位置识别] 使用多相机融合模式 ({1 + len(extra_images)} 个视角)")

        # 水平拼接所有图像
        all_images = [global_image] + extra_images
        target_h = min(img.shape[0] for img in all_images)
        resized_images = []
        for img in all_images:
            scale = target_h / img.shape[0]
            new_w = int(img.shape[1] * scale)
            resized = cv2.resize(img, (new_w, target_h))
            resized_images.append(resized)
        combined_image = np.hstack(resized_images)

        # 保存拼接图像用于调试
        cv2.imwrite("debug_combined_views.jpg", combined_image)
```

**主控制端调用（main_vlm.py:241-275）：**
```python
# 获取多个全局相机图像
imgs_global_2 = env.render(camera_name=CAMERA_GLOBAL_2)
color_global = cv2.cvtColor(imgs_global_2['img'], cv2.COLOR_RGB2BGR)

# 额外相机 (用于辅助识别)
extra_images = []
try:
    imgs_global_1 = env.render(camera_name=CAMERA_GLOBAL_1)
    color_global_1 = cv2.cvtColor(imgs_global_1['img'], cv2.COLOR_RGB2BGR)
    extra_images.append(color_global_1)
except:
    print("  cam_global_1 不可用")

# VLM识别放置位置 (传入多相机图像)
place_result = detect_place_position(place_description, color_global, extra_images=extra_images)
```

#### 3.2.3 颜色区域识别

**功能：** 支持识别桌面上的彩色标记区域

```python
# 检查是否是颜色区域描述
color_pattern = r'(红色|绿色|蓝色|黄色|白色|黑色|橙色|紫色)(的)?(区域|地方|位置|部分)'
color_match = re.search(color_pattern, place_description)

if color_match:
    color_region = color_match.group(1)
    print(f"[放置位置识别] 检测到颜色区域描述: {color_region}")

    # 颜色映射（中文到英文）
    color_map = {
        "红色": "red", "绿色": "green", "蓝色": "blue",
        "黄色": "yellow", "白色": "white", "黑色": "black",
        "橙色": "orange", "紫色": "purple",
    }

    # 构建专门的颜色识别提示词
    color_prompt = f"""请在图像中找到 {color_region}/{color_en} 颜色的区域..."""
```

#### 3.2.4 距离描述解析

**功能：** 根据用户描述中的距离信息，动态调整偏移量

```python
def parse_distance_offset(description):
    """
    根据描述中的距离信息计算像素偏移量。
    相机视角下，大约 1cm ≈ 8-12 像素（取决于深度）
    """
    # 检查明确的厘米数值
    cm_match = re.search(r'(\d+)[-~到]?(\d*)(?:cm|厘米)', description)
    if cm_match:
        cm_min = int(cm_match.group(1))
        cm_max = int(cm_match.group(2)) if cm_match.group(2) else cm_min
        avg_cm = (cm_min + cm_max) / 2
        offset = int(avg_cm * 10)  # 大约 10 像素/厘米
        return max(20, min(200, offset))

    # 检查相对距离描述
    close_keywords = ['紧挨', '紧贴', '贴着', '挨着', '很近', '靠近']
    medium_keywords = ['旁边', '边上', '附近']
    far_keywords = ['远一点', '远些', '离远', '稍远']

    for keyword in close_keywords:
        if re.search(keyword, description):
            return 40  # 近距离

    for keyword in medium_keywords:
        if re.search(keyword, description):
            return 70  # 中等距离

    for keyword in far_keywords:
        if re.search(keyword, description):
            return 120  # 远距离

    return 80  # 默认偏移
```

---

### 3.3 像素到世界坐标转换 (pixel_to_world)

**功能描述：**
将VLM识别的像素坐标转换为机械臂可以使用的世界坐标。

**实现位置：** `vlm_process.py:416-454`

**核心代码：**
```python
def pixel_to_world(pixel_x, pixel_y, depth_img, T_wc, fovy, img_shape):
    """
    将像素坐标转换为世界坐标。

    参数:
        pixel_x, pixel_y: 像素坐标
        depth_img: 深度图像
        T_wc: 相机到世界的变换矩阵 (spatialmath.SE3)
        fovy: 垂直视场角 (弧度)
        img_shape: 图像尺寸 (height, width)

    返回:
        world_point: [x, y, z] 世界坐标
    """
    height, width = img_shape[:2]

    # 计算相机内参
    focal = height / (2.0 * np.tan(fovy / 2.0))
    cx = width / 2.0
    cy = height / 2.0

    # 获取深度值
    depth = depth_img[int(pixel_y), int(pixel_x)]

    if depth <= 0 or np.isnan(depth) or np.isinf(depth):
        print(f"[警告] 深度值无效: {depth}，使用默认桌面高度")
        depth = 2.5  # 估计深度

    # 反投影到相机坐标系
    x_c = (pixel_x - cx) * depth / focal
    y_c = (pixel_y - cy) * depth / focal
    z_c = depth

    # 转换到世界坐标系
    point_camera = np.array([x_c, y_c, z_c, 1.0])
    point_world = T_wc.A @ point_camera

    return point_world[:3]
```

**坐标转换原理：**

```
像素坐标 (u, v)
      ↓ 反投影（使用相机内参）
相机坐标 (x_c, y_c, z_c)
      ↓ 变换矩阵 T_wc
世界坐标 (x_w, y_w, z_w)
```

---

### 3.4 抓取执行优化 (execute_grasp)

**功能描述：**
执行完整的抓取-搬运-放置动作序列，支持动态放置位置。

**实现位置：** `grasp_process_optimized.py:694-1398`

#### 3.4.1 动态放置位置支持

**修改前：** 放置位置硬编码为固定值
```python
target_pos = [0.2, 0.2, 0.92]  # 固定位置
```

**修改后：** 支持传入动态放置位置
```python
def execute_grasp(env, gg, T_wc=None, target_pos=None):
    """
    参数:
        target_pos (list): 放置位置 [x, y, z]，如果为None则使用默认位置。
    """
    if target_pos is None:
        target_pos = [0.2, 0.2, 0.92]  # 默认背后位置
        print(f"[PLACE] 使用默认放置位置: {target_pos}")
    else:
        print(f"[PLACE] 使用指定放置位置: {target_pos}")
```

#### 3.4.2 区域判断策略

根据放置位置的不同区域，采用不同的搬运策略：

```python
# 区域划分（俯视图）：
#        Y轴
#        ^
#   1.2  +------------------+
#        |   左后方区域      |  (需要转身，关节空间)
#        |   x<0.5, y>0.5   |
#   0.5  +--------+---------+
#        | 背后   | 中间区域 |  侧面/货架方向
#        | x<0.5  | 0.5<=x  |  x>=1.2
#        | y<0.5  | <1.2    |  (保持原姿态)
#   0    +--------+---------+-----> X轴
#        0       0.5       1.2

needs_turn_around = (target_pos[0] < 0.5)  # x < 0.5 需要转身
is_middle_area = (0.5 <= target_pos[0] < 1.2 and not needs_turn_around)
is_side_area = (target_pos[0] >= 1.2)  # 货架方向
```

**策略选择：**
- **需要转身区域 (x < 0.5)**：使用关节空间插值，启用水平保持补偿
- **中间区域**：使用关节空间插值，垂直向下姿态
- **侧面/货架方向**：使用笛卡尔直线规划，保持抓取姿态

#### 3.4.3 工作空间边界检查

**主控制端实现（main_vlm.py:316-330）：**
```python
# 检查并调整到机械臂工作空间内
WORKSPACE_X_MIN, WORKSPACE_X_MAX = 0.1, 1.0
WORKSPACE_Y_MIN, WORKSPACE_Y_MAX = 0.1, 0.9

original_pos = target_pos.copy()
target_pos[0] = max(WORKSPACE_X_MIN, min(WORKSPACE_X_MAX, target_pos[0]))
target_pos[1] = max(WORKSPACE_Y_MIN, min(WORKSPACE_Y_MAX, target_pos[1]))

if original_pos[0] != target_pos[0] or original_pos[1] != target_pos[1]:
    print(f"  ⚠️ 原始位置超出工作空间，已调整!")
```

#### 3.4.4 夹爪控制优化

**问题：** 机械臂复位时，夹爪默认闭合导致重新抓取已放置的物品

**解决方案：** 在复位阶段显式保持夹爪打开

```python
# 7.抬起夹爪 - 保持夹爪打开
for timei in times:
    # ... 运动规划代码 ...
    action[-1] = 0  # 保持夹爪打开
    env.step(action)

# 8.回到初始位置 - 保持夹爪打开
for timei in times:
    # ... 运动规划代码 ...
    action[-1] = 0  # 保持夹爪打开
    env.step(action)
```

---

## 4. 遇到的问题与解决方案

### 4.1 VLM无法识别显微镜

**问题描述：**
当用户指令为"把培养皿放到显微镜的右边"时，VLM在单个相机视角下无法准确识别显微镜的位置。

**原因分析：**
- 显微镜可能被其他物体部分遮挡
- 单一视角的信息不足以准确定位

**解决方案：多相机融合**

```python
# main_vlm.py - 获取多个全局相机图像
extra_images = []
try:
    imgs_global_1 = env.render(camera_name=CAMERA_GLOBAL_1)
    color_global_1 = cv2.cvtColor(imgs_global_1['img'], cv2.COLOR_RGB2BGR)
    extra_images.append(color_global_1)
except:
    print("  cam_global_1 不可用")

# 传入多相机图像进行识别
place_result = detect_place_position(place_description, color_global, extra_images=extra_images)
```

```python
# vlm_process.py - 拼接多视角图像
if extra_images and len(extra_images) > 0:
    print(f"[放置位置识别] 使用多相机融合模式 ({1 + len(extra_images)} 个视角)")
    all_images = [global_image] + extra_images
    combined_image = np.hstack(resized_images)
```

**效果：** 通过多视角融合，VLM可以综合多个角度的信息，显著提高了显微镜等物体的识别准确率。

---

### 4.2 放置位置超出工作空间

**问题描述：**
VLM识别的放置位置可能超出机械臂的可达范围，导致IK求解失败或机械臂运动异常。

**原因分析：**
- 全局相机视野范围大于机械臂工作空间
- VLM不了解机械臂的物理限制

**解决方案：工作空间边界检查**

```python
# main_vlm.py:316-330
# 检查并调整到机械臂工作空间内
# UR5e机械臂基座在原点附近，工作半径约0.85m
WORKSPACE_X_MIN, WORKSPACE_X_MAX = 0.1, 1.0
WORKSPACE_Y_MIN, WORKSPACE_Y_MAX = 0.1, 0.9

original_pos = target_pos.copy()
target_pos[0] = max(WORKSPACE_X_MIN, min(WORKSPACE_X_MAX, target_pos[0]))
target_pos[1] = max(WORKSPACE_Y_MIN, min(WORKSPACE_Y_MAX, target_pos[1]))

if original_pos[0] != target_pos[0] or original_pos[1] != target_pos[1]:
    print(f"  ⚠️ 原始位置超出工作空间，已调整!")
    print(f"  调整后坐标: ({target_pos[0]:.3f}, {target_pos[1]:.3f}, {target_pos[2]:.3f})")
```

**效果：** 即使VLM识别的位置超出范围，系统也会自动调整到最近的可达位置，避免执行失败。

---

### 4.3 夹爪复位时重新抓取物品

**问题描述：**
机械臂放置物品后复位时，夹爪会自动闭合，导致重新抓取刚放下的物品。

**原因分析：**
- `action[-1]` 默认值为0，但在某些代码路径中未显式设置
- 复位阶段的代码没有考虑夹爪状态

**解决方案：显式控制夹爪状态**

```python
# grasp_process_optimized.py
# 7.抬起夹爪 - 显式保持夹爪打开
for timei in times:
    for j in range(len(time_cumsum)):
        # ... 运动规划代码 ...
        action[:6] = joint
        action[-1] = 0  # 关键：保持夹爪打开
        env.step(action)

# 8.回到初始位置 - 显式保持夹爪打开
for timei in times:
    for j in range(len(time_cumsum)):
        # ... 运动规划代码 ...
        action[:6] = joint
        action[-1] = 0  # 关键：保持夹爪打开
        env.step(action)
```

**效果：** 机械臂在整个复位过程中保持夹爪打开，不会重新抓取物品。

---

### 4.4 距离描述被忽略

**问题描述：**
用户指令中的距离描述（如"紧挨着"、"5cm"）被忽略，放置位置总是使用固定偏移。

**原因分析：**
- 原始实现使用固定的像素偏移量
- 没有解析用户指令中的距离信息

**解决方案：距离解析函数**

```python
# vlm_process.py:187-225
def parse_distance_offset(description):
    """根据描述中的距离信息计算像素偏移量"""

    # 1. 检查明确的厘米数值
    cm_match = re.search(r'(\d+)[-~到]?(\d*)(?:cm|厘米)', description)
    if cm_match:
        cm_min = int(cm_match.group(1))
        cm_max = int(cm_match.group(2)) if cm_match.group(2) else cm_min
        avg_cm = (cm_min + cm_max) / 2
        offset = int(avg_cm * 10)  # 大约 10 像素/厘米
        print(f"[距离解析] 检测到距离: {cm_min}-{cm_max}cm → 偏移 {offset} 像素")
        return max(20, min(200, offset))

    # 2. 检查相对距离描述
    close_keywords = ['紧挨', '紧贴', '贴着', '挨着', '很近', '靠近']
    for keyword in close_keywords:
        if re.search(keyword, description):
            print(f"[距离解析] 检测到近距离关键词: '{keyword}' → 偏移 40 像素")
            return 40

    medium_keywords = ['旁边', '边上', '附近']
    for keyword in medium_keywords:
        if re.search(keyword, description):
            print(f"[距离解析] 检测到中等距离关键词: '{keyword}' → 偏移 70 像素")
            return 70

    far_keywords = ['远一点', '远些', '离远', '稍远']
    for keyword in far_keywords:
        if re.search(keyword, description):
            print(f"[距离解析] 检测到远距离关键词: '{keyword}' → 偏移 120 像素")
            return 120

    print(f"[距离解析] 未检测到距离描述，使用默认偏移 80 像素")
    return 80
```

**效果：** 系统可以理解并响应用户的距离描述，实现更精确的放置控制。

---

### 4.5 颜色区域无法识别

**问题描述：**
用户指令中包含颜色区域描述（如"红色区域"）时，VLM无法准确识别。

**原因分析：**
- 通用的放置位置识别提示词不够针对颜色区域
- 需要专门的颜色识别逻辑

**解决方案：专门的颜色区域识别**

```python
# vlm_process.py:232-290
# 检查是否是颜色区域描述
color_pattern = r'(红色|绿色|蓝色|黄色|白色|黑色|橙色|紫色)(的)?(区域|地方|位置|部分)'
color_match = re.search(color_pattern, place_description)

if color_match:
    color_region = color_match.group(1)
    print(f"[放置位置识别] 检测到颜色区域描述: {color_region}")
    print(f"[放置位置识别] 使用颜色区域识别模式...")

    # 颜色映射（中文到英文）
    color_map = {
        "红色": "red", "绿色": "green", "蓝色": "blue",
        "黄色": "yellow", "白色": "white", "黑色": "black",
        "橙色": "orange", "紫色": "purple",
    }
    color_en = color_map.get(color_region, color_region)

    # 构建专门的颜色识别提示词
    color_prompt = f"""请在图像中找到 {color_region}/{color_en} 颜色的区域，并返回该区域的中心点坐标。

【重要提示】
- 仔细观察桌面/工作台上的颜色标记区域
- {color_region}区域通常是桌面上的彩色标记或贴纸
- 图像尺寸: {w} x {h} 像素
- 坐标系: 左上角(0,0)，x向右增大，y向下增大

请返回JSON格式：
{{"found": true, "center": [x, y], "reason": "找到{color_region}区域的原因"}}
如果找不到{color_region}区域，返回：
{{"found": false, "reason": "未找到的原因"}}"""
```

**效果：** 系统可以准确识别桌面上的彩色标记区域，支持"放到红色区域"等指令。

---

## 5. 使用示例

### 5.1 启动系统

```bash
cd /home/robot/VLM_Grasp_Bio
python main_vlm.py
```

### 5.2 选择智能放置模式

```
📷 当前模式: SINGLE
   输入 '1' 单相机模式 - 桌面相机 (cam)
   输入 '2' 单相机模式 - 货架相机 (cam_shelf)
   输入 '3' 融合模式 - 双相机点云融合
   输入 '4' 智能放置模式 - 自然语言指定放置位置 🔥
   直接按回车继续...

选择: 4
✅ 智能放置模式: 支持自然语言指定放置位置
```

### 5.3 输入自然语言指令

```
============================================================
🤖 智能放置模式
============================================================
请输入完整的自然语言指令，例如：
  - 把培养皿放置到显微镜的右边
  - 把试管移到桌子左上角
  - 抓取烧杯放到红色区域
============================================================

请输入指令: 把培养皿放置到显微镜的右边
```

### 5.4 系统执行流程

```
[Step 1] 解析用户指令...
[指令解析] 原始响应: {"grasp_target": "培养皿", "place_description": "显微镜的右边", "has_place_instruction": true}
  抓取目标: 培养皿
  放置位置: 显微镜的右边

[Step 2] 获取桌面相机图像...

[Step 3] VLM识别抓取目标...
[DEBUG] VLM 增强提示词: 培养皿 (green cylinder, small container, cup)
原始响应：好的，我识别到了培养皿...
✅ 自动检测到目标,bbox:[245, 312, 298, 365]

[Step 4] 抓取姿态推理...
[DEBUG] Filtered 15 grasps within ±30° of vertical out of 128 total predictions.
✅ 已自动执行抓取头调平优化 (Orientation Auto-leveled)

[Step 5] 识别放置位置...
  获取多视角全局相机图像...
  使用 3 个相机视角
[放置位置识别] 使用多相机融合模式 (3 个视角)
[放置位置识别] 解析结果: 参考物体='显微镜', 方向='右边', 偏移=80像素
[放置位置识别] 第一阶段：识别参考物体 '显微镜'...
[放置位置识别] 找到 '显微镜' 在 (412, 156)
  VLM识别的放置位置: 像素坐标 (492, 156)
  世界坐标: (0.856, 0.423, 0.760)

[Step 6] 执行抓取和放置...
[PLACE] 使用指定放置位置: [0.856, 0.423, 0.76]
📦 [TABLE GRASP] 检测到桌面抓取，使用标准直线接近策略

✅ 任务完成!
```

### 5.5 支持的指令格式

| 指令类型 | 示例 |
|----------|------|
| 基本抓取 | "抓取培养皿" |
| 相对位置放置 | "把培养皿放到显微镜的右边" |
| 颜色区域放置 | "把试管放到红色区域" |
| 带距离描述 | "把烧杯放到显微镜旁边，紧挨着" |
| 精确距离 | "把培养皿放到显微镜右边5cm处" |

---

## 6. 待改进项

### 6.1 鸭子抓取问题

**现状：** 鸭子（黄色橡皮鸭）的抓取仍然存在问题，VLM识别准确率较低。

**可能原因：**
- 鸭子的形状不规则，边界框不准确
- 黄色物体在某些光照条件下识别困难
- SAM分割对不规则形状物体效果不佳

**改进方向：**
1. 针对鸭子添加专门的提示词增强
2. 调整SAM分割参数
3. 考虑使用多次识别取平均的策略

### 6.2 其他已知限制

1. **深度图噪声**：某些区域的深度值可能不准确，影响坐标转换精度
2. **遮挡处理**：当目标物体被严重遮挡时，识别可能失败
3. **动态场景**：当前系统假设场景静态，不支持动态物体追踪
4. **多物体抓取**：一次只能抓取一个物体，不支持批量操作

### 6.3 未来优化方向

1. **引入物体追踪**：支持动态场景下的持续追踪
2. **学习用户偏好**：记录用户的放置习惯，提供更智能的默认位置
3. **多轮对话**：支持用户通过对话修正放置位置
4. **力反馈集成**：结合力传感器实现更精细的放置控制

---

## 附录：关键文件索引

| 文件 | 主要功能 |
|------|----------|
| `main_vlm.py` | 主控制程序，流程编排 |
| `vlm_process.py` | VLM相关功能（指令解析、放置识别、坐标转换） |
| `grasp_process_optimized.py` | 抓取推理和执行 |
| `manipulator_grasp/env/ur5_grasp_env.py` | MuJoCo仿真环境 |
| `manipulator_grasp/arm/motion_planning.py` | 运动规划模块 |

---

*文档版本: 1.0*
*最后更新: 2026-02-07*
