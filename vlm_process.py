import cv2
import numpy as np
import torch
from ultralytics.models.sam import Predictor as SAMPredictor

# import whisper
import json
import re
import base64
import textwrap
import queue
import time
import io
import os  # å¯¼å…¥ os ç”¨äºå¤„ç†ç¯å¢ƒå˜é‡

# å¼ºåˆ¶æ¸…é™¤å¯èƒ½å¯¼è‡´é”™è¯¯çš„ä»£ç†ç¯å¢ƒå˜é‡
for key in ['all_proxy', 'ALL_PROXY', 'http_proxy', 'HTTP_PROXY', 'https_proxy', 'HTTPS_PROXY']:
    os.environ.pop(key, None)

# import soundfile as sf  
# import sounddevice as sd
# from scipy.io.wavfile import write
# from pydub import AudioSegment

from openai import OpenAI  # å¯¼å…¥OpenAIå®¢æˆ·ç«¯
import httpx  # å¯¼å…¥ httpx ç”¨äºå¤„ç†ä»£ç†é—®é¢˜

import logging
# ç¦ç”¨ Ultralytics çš„æ—¥å¿—è¾“å‡º
logging.getLogger("ultralytics").setLevel(logging.WARNING)

from google import genai
from google.genai import types


# ----------------------- åŸºç¡€å·¥å…·å‡½æ•° -----------------------

def encode_np_array(image_np):
    """å°† numpy å›¾åƒæ•°ç»„ï¼ˆBGRï¼‰ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    success, buffer = cv2.imencode('.jpg', image_np)
    if not success:
        raise ValueError("æ— æ³•å°†å›¾åƒæ•°ç»„ç¼–ç ä¸º JPEG")
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64



# ----------------------- å¤šæ¨¡æ€æ¨¡å‹è°ƒç”¨ï¼ˆQwenï¼‰ -----------------------

def generate_robot_actions(user_command, image_input=None):
    """
    ä½¿ç”¨ base64 çš„æ–¹å¼å°† numpy å›¾åƒå’Œç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤ä¼ ç»™ Qwen å¤šæ¨¡æ€æ¨¡å‹ï¼Œ
    è¦æ±‚æ¨¡å‹è¿”å›ä¸¤éƒ¨åˆ†ï¼š
      - æ¨¡å‹è¿”å›å†…å®¹ä¸­ï¼Œç¬¬ä¸€éƒ¨åˆ†ä¸ºè‡ªç„¶è¯­è¨€å“åº”ï¼ˆè¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“ï¼‰ï¼Œ
      - ç´§è·Ÿå…¶åçš„éƒ¨åˆ†ä¸ºçº¯ JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

        {
          "name": "ç‰©ä½“åç§°",
          "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
        }

    è¿”å›ä¸€ä¸ª dictï¼ŒåŒ…å« "response" å’Œ "coordinates"ã€‚
    å‚æ•° image_input ä¸º numpy æ•°ç»„ï¼ˆBGR æ ¼å¼ï¼‰ã€‚
    """
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œå½»åº•ç¦ç”¨ç¯å¢ƒä»£ç† (trust_env=False)
    # æ›¿æ¢ä¸ºè‡ªå·±çš„æ¨¡å‹è°ƒç”¨ï¼Œæ²¡æœ‰æœ¬åœ°éƒ¨ç½²çš„ï¼Œå¯ä»¥å‚è€ƒè¯¥ç½‘ç«™ https://sg.uiuiapi.com/v1
    client = OpenAI(
        api_key='OPENAI_API_KEY', 
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        http_client=httpx.Client(trust_env=False)
    )       
    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ï¼Œå¹¶è®°å½•æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’ç‚¹å’Œå³ä¸‹è§’ç‚¹ï¼‰åŠå…¶ç±»åˆ«åç§°ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**,ä½†æ˜¯ä¸è¦è¿”å›jsonæœ¬ä½“,æ ¼å¼å¦‚ä¸‹ï¼š

    {
      "name": "ç‰©ä½“åç§°",
      "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
    }

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - JSON å¿…é¡»ä»ä¸‹ä¸€è¡Œå¼€å§‹ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
    - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š,åŒ…æ‹¬ä¸èƒ½æœ‰è¾…åŠ©æ ‡è¯†ä¸ºjsonæ–‡æœ¬çš„å†…å®¹,ä¸è¦æœ‰json;
    - åæ ‡ bbox å¿…é¡»ä¸ºæ•´æ•°ï¼›
    - åªå…è®¸ä½¿ç”¨ "bbox" ä½œä¸ºåæ ‡æ ¼å¼ã€‚
    """)

    messages = [{"role": "system", "content": system_prompt}]
    user_content = []

    if image_input is not None:
        base64_img = encode_np_array(image_input)
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_img}"
            }
        })

    user_content.append({"type": "text", "text": user_command})
    messages.append({"role": "user", "content": user_content})

    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨API
        completion = client.chat.completions.create(
            model="qwen-vl-max-latest", 
            # model="gpt-5.2-2025-12-11",  # æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¯·ç¡®è®¤æœåŠ¡æä¾›å•†æ”¯æŒçš„æ¨¡å‹å
            # qwen3-omni-flash"
            # model="qwen-vl-plus",
            # model="qwen-vl-max", 
            # model="gpt-5",
            # model="qwen2.5-vl-32b-instruct",
            messages=messages,
            # max_tokens=4096,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
            temperature=0.1,   # é™ä½æ¸©åº¦ä»¥æé«˜è¾“å‡ºçš„ç¡®å®šæ€§ï¼Œå¯¹ç»“æ„åŒ–è¾“å‡ºæœ‰ç›Š
        )
        
        content = completion.choices[0].message.content
        print("åŸå§‹å“åº”ï¼š", content)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            json_str = match.group(1)
            try:
                coord = json.loads(json_str)
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                coord = {}
            natural_response = content[:match.start()].strip()
        else:
            natural_response = content.strip()
            coord = {}

        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": "å¤„ç†å¤±è´¥", "coordinates": {}}


def generate_robot_actions_gemini(user_command, image_input=None):
    """
    ä½¿ç”¨ Google Gemini Robotics-ER 1.5 æ¨¡å‹å¤„ç†å›¾åƒå’ŒæŒ‡ä»¤ã€‚
    ä½¿ç”¨ä¸åŸ Qwen/OpenAI ç›¸åŒçš„æç¤ºè¯é€»è¾‘ï¼Œä½†é€‚é… Gemini çš„è¾“å…¥è¾“å‡ºã€‚
    """
    # æ›¿æ¢ä¸ºç”¨æˆ·çš„ API Key
    client = genai.Client(api_key='AIzaSyBJvmAHO92kO4t0zKo7sqrtrnk9jmR3HRk')
    MODEL_ID = "gemini-robotics-er-1.5-preview"

    if image_input is None:
        return {"response": "éœ€è¦å›¾åƒè¾“å…¥", "coordinates": {}}

    # å°† numpy BGR å›¾åƒè½¬ä¸º RGB å¹¶ç¼–ç ä¸º bytes
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)
    success, encoded_image = cv2.imencode('.jpg', image_rgb)
    if not success:
         return {"response": "å›¾åƒç¼–ç å¤±è´¥", "coordinates": {}}
    image_bytes = encoded_image.tobytes()

    # å¤ç”¨åŸæœ‰çš„ System Prompt é€»è¾‘ï¼Œä¿æŒå®éªŒä¸€è‡´æ€§
    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ ({user_command})ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**, æ ¼å¼å¦‚ä¸‹ï¼š

    [
      {{
        "box_2d": [ymin, xmin, ymax, xmax],
        "label": "ç‰©ä½“åç§°"
      }}
    ]

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - åæ ‡ box_2d å¿…é¡»ä¸º 0-1000 çš„å½’ä¸€åŒ–æ•´æ•° (Gemini æ ‡å‡†)ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
    - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€‚
    """)

    # ç»„åˆ Prompt
    full_prompt = system_prompt.format(user_command=user_command)

    try:
        response = client.models.generate_content(
            model=MODEL_ID,
            contents=[
                types.Part.from_bytes(
                    data=image_bytes,
                    mime_type='image/jpeg',
                ),
                full_prompt
            ],
            config = types.GenerateContentConfig(
                temperature=0.5,
                thinking_config=types.ThinkingConfig(thinking_budget=1024) 
            )
        )
        
        content = response.text
        print("Gemini åŸå§‹å“åº”ï¼š", content)

        # è§£æå“åº”
        # å¯»æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\[.*\])', content, re.DOTALL)
        natural_response = content
        coord = {}

        if match:
            json_str = match.group(1)
            natural_response = content[:match.start()].strip()
            try:
                items = json.loads(json_str)
                if items and len(items) > 0:
                    item = items[0]
                    #å¤„ç†åæ ‡è½¬æ¢ï¼šGemini (0-1000) -> åƒç´ 
                    h, w = image_input.shape[:2]
                    box_2d = item.get("box_2d")
                    
                    if box_2d:
                        ymin, xmin, ymax, xmax = box_2d
                        x1 = int(xmin / 1000 * w)
                        y1 = int(ymin / 1000 * h)
                        x2 = int(xmax / 1000 * w)
                        y2 = int(ymax / 1000 * h)
                        
                        coord = {
                            "name": item.get("label", "target"),
                            "bbox": [x1, y1, x2, y2]
                        }
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                # å°è¯•ç¨å¾®æ¸…æ´—ä¸‹ json å†è§£æ
        
        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"Gemini è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": f"å¤„ç†å¤±è´¥: {e}", "coordinates": {}}


# ----------------------- SAM åˆ†å‰²ç›¸å…³ -----------------------
def choose_model():
    """Initialize SAM predictor with proper parameters"""
    model_weight = 'sam_b.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        # imgsz=1024,
        model=model_weight,
        conf=0.25,
        save=False
    )
    return SAMPredictor(overrides=overrides)

def process_sam_results(results):
    """Process SAM results to get mask and center point"""
    if not results or not results[0].masks:
        return None, None

    # Get first mask (assuming single object segmentation)
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255

    # Find contour and center
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None

    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask

    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


# ----------------------- è¯­éŸ³è¯†åˆ«ä¸ TTS -----------------------

# åˆå§‹åŒ–å…¨å±€æ¨¡å‹å˜é‡
_global_models = {}


def load_models():
    """åœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹ï¼Œé¿å…å¯åŠ¨æ—¶å…¨éƒ¨åŠ è½½å ç”¨èµ„æº"""
    if not _global_models:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç¦»çº¿è¯­éŸ³æ¨¡å‹...")
        # åŠ è½½Whisperå°å‹æ¨¡å‹ (é€‚åˆä½ çš„6GBæ˜¾å­˜)
        # _global_models['asr'] = whisper.load_model("small")
        # _global_models['asr'] = whisper.load_model("tiny")
        # _global_models['asr'] = whisper.load_model("base")
        print("âœ… Whisperçš„baseæ¨¡å‹åŠ è½½å®Œæ¯•")

        try:
            import pyttsx3
            _global_models['tts_backup'] = pyttsx3.init()
            # é…ç½®TTS
            _global_models['tts_backup'].setProperty('rate', 160)  # è¯­é€Ÿ
            voices = _global_models['tts_backup'].getProperty('voices')
            for voice in voices:
                if 'chinese' in voice.name.lower() or 'zh' in voice.id.lower():
                    _global_models['tts_backup'].setProperty('voice', voice.id)
                    break
            print("âœ… TTS (pyttsx3) åˆå§‹åŒ–å®Œæ¯•")
        except Exception as e:
            print(f"âš ï¸  TTSåˆå§‹åŒ–å¤±è´¥: {e}")
            _global_models['tts_backup'] = None

    return _global_models


# éŸ³é¢‘å‚æ•°é…ç½®
samplerate = 16000
channels = 1
dtype = 'int16'
frame_duration = 0.2
frame_samples = int(frame_duration * samplerate)
silence_threshold = 250
silence_max_duration = 2.0
q = queue.Queue()


def rms(audio_frame):
    samples = np.frombuffer(audio_frame, dtype=np.int16)
    if samples.size == 0:
        return 0
    mean_square = np.mean(samples.astype(np.float32) ** 2)
    if np.isnan(mean_square) or mean_square < 1e-5:
        return 0
    return np.sqrt(mean_square)

def callback(indata, frames, time_info, status):
    if status:
        print("âš ï¸ çŠ¶æ€è­¦å‘Šï¼š", status)
    q.put(bytes(indata))

def recognize_speech():
    """å½•éŸ³å¹¶è¿”å›éŸ³é¢‘æ•°æ®ï¼ˆnumpy æ•°ç»„ï¼‰"""
    print("ğŸ™ï¸ å¯åŠ¨å½•éŸ³ï¼Œè¯·è¯´è¯...")
    # print("ğŸ’¡ è°ƒè¯•ä¿¡æ¯ï¼šæ­£åœ¨ç›‘æµ‹å®æ—¶éŸ³é‡ï¼ˆRMSï¼‰ï¼Œè¯·è§‚å¯Ÿä¸è¯´è¯æ—¶çš„åŸºç¡€å™ªéŸ³å€¼")
    audio_buffer = []
    is_speaking = False
    last_voice_time = time.time()

    with sd.RawInputStream(samplerate=samplerate, blocksize=frame_samples,
                           dtype=dtype, channels=channels, callback=callback):
        while True:
            frame = q.get()
            volume = rms(frame)
            current_time = time.time()

            # print(f"å®æ—¶éŸ³é‡ï¼ˆRMSï¼‰: {volume}") 

            if volume > silence_threshold:
                if not is_speaking:
                    print("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
                    is_speaking = True
                    audio_buffer = []
                audio_np = np.frombuffer(frame, dtype=np.int16)
                audio_buffer.append(audio_np)
                last_voice_time = current_time
            elif is_speaking and (current_time - last_voice_time > silence_max_duration):
                print("ğŸ›‘ åœæ­¢å½•éŸ³ï¼Œå‡†å¤‡è¯†åˆ«...")
                full_audio = np.concatenate(audio_buffer, axis=0)
                return full_audio
            elif not is_speaking and (current_time - last_voice_time > 10.0):
                print("ğŸ›‘ è¶…æ—¶ï¼šæœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥")
                return np.array([], dtype=np.int16)

def speech_to_text_offline(audio_data):
    """
    ä½¿ç”¨ç¦»çº¿Whisperæ¨¡å‹å°†å½•éŸ³æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬
    """
    print("ğŸ“¡ æ­£åœ¨è¿›è¡Œç¦»çº¿è¯­éŸ³è¯†åˆ«...")
    models = load_models()
    asr_model = models['asr']

    # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    temp_wav = "temp_audio.wav"
    write(temp_wav, samplerate, audio_data.astype(np.int16))

    try:
        # ä½¿ç”¨Whisperè¿›è¡Œè¯†åˆ«ï¼ŒæŒ‡å®šè¯­è¨€ä¸ºä¸­æ–‡ä»¥æé«˜ç²¾åº¦å’Œé€Ÿåº¦
        result = asr_model.transcribe(temp_wav, language="zh", fp16=torch.cuda.is_available())
        return result["text"].strip()
    except Exception as e:
        print(f"âŒ ç¦»çº¿è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        return ""

def play_tts_offline(text):
    """
    ä½¿ç”¨ç¦»çº¿TTSæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶æ’­æ”¾
    """
    if not text:
        return
        
    print(f"ğŸ“¢ ç¦»çº¿TTSæ’­æ”¾: {text}")
    models = load_models()

    try:
        if models['tts_backup'] is not None:
            models['tts_backup'].say(text)
            models['tts_backup'].runAndWait()

    except Exception as e:
        print("âŒ æ— å¯ç”¨TTSå¼•æ“")


def voice_command_to_keyword():
    """
    è·å–è¯­éŸ³å‘½ä»¤å¹¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
    ç›´æ¥è¿”å›è¯†åˆ«çš„æ–‡æœ¬æŒ‡ä»¤ã€‚
    """
    audio_data = recognize_speech()
    text = speech_to_text_offline(audio_data) # æ”¹ä¸ºè°ƒç”¨ç¦»çº¿ASR
    if not text:
        print("âš ï¸ æ²¡æœ‰è¯†åˆ«åˆ°æ–‡æœ¬")
        return ""
    print("ğŸ“ è¯†åˆ«æ–‡æœ¬ï¼š", text)
    # play_tts_offline(f"å·²æ”¶åˆ°æŒ‡ä»¤: {text}") # æ”¹ä¸ºè°ƒç”¨ç¦»çº¿TTS
    return text


# ----------------------- ä¸»æµç¨‹ï¼šå›¾åƒåˆ†å‰² -----------------------
def segment_image(image_input, output_mask='mask1.png', command_text=None):
    # 1. ä½¿ç”¨æ–‡å­—è·å–ç›®æ ‡æŒ‡ä»¤
    if command_text is None:
        print("ğŸ“ è¯·é€šè¿‡æ–‡å­—æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
        command_text = input("è¯·è¾“å…¥: ").strip()
    
    if not command_text:
        print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
        # è¿”å›é»‘è‰²çš„å…¨é›¶æ©ç ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
        h, w = image_input.shape[:2]
        return np.zeros((h, w), dtype=np.uint8)
        
    print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # # 1. ä½¿ç”¨è¯­éŸ³è·å–ç›®æ ‡æŒ‡ä»¤
    # print("ğŸ™ï¸ è¯·é€šè¿‡è¯­éŸ³æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
    # command_text = voice_command_to_keyword()
    # if not command_text:
    #     print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
    #     return None
    # print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    # --- Prompt Enhancing: è‡ªåŠ¨è¡¥å……è§†è§‰æè¿°ä»¥æé«˜è¯†åˆ«ç‡ ---
    enhanced_command = command_text
    if "åŸ¹å…»çš¿" in command_text:
        enhanced_command = f"{command_text} (green cylinder, small container, cup)"
    
    print(f"[DEBUG] VLM å¢å¼ºæç¤ºè¯: {enhanced_command}")
    
    result = generate_robot_actions(enhanced_command, image_input)
    # åˆ‡æ¢ä¸º Gemini æ¨¡å‹
    # result = generate_robot_actions_gemini(command_text, image_input)
    natural_response = result["response"]
    detection_info = result["coordinates"]
    print("è‡ªç„¶è¯­è¨€å›åº”ï¼š", natural_response)
    print("æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯ï¼š", detection_info)

    # ä»…å¯¹æ¨¡å‹è¿”å›çš„è‡ªç„¶è¯­è¨€å›åº”æ’­æŠ¥
    # play_tts_offline(natural_response)
    
    bbox = detection_info.get("bbox") if detection_info and "bbox" in detection_info else None
    
    # 3. å‡†å¤‡å›¾åƒä¾› SAM ä½¿ç”¨ï¼ˆè½¬æ¢ä¸º RGBï¼‰
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)

    # 4. åˆå§‹åŒ– SAMï¼Œå¹¶è®¾ç½®å›¾åƒ
    predictor = choose_model()
    predictor.set_image(image_rgb)

    if bbox:
        results = predictor(bboxes=[bbox])
        center, mask = process_sam_results(results)
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡,bbox:{bbox}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¯·ç‚¹å‡»å›¾åƒé€‰æ‹©å¯¹è±¡")
        cv2.namedWindow('Select Object', cv2.WINDOW_NORMAL)
        cv2.imshow('Select Object', image_input)
        point = []

        def click_handler(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                point.extend([x, y])
                print(f"ğŸ–±ï¸ ç‚¹å‡»åæ ‡ï¼š{x}, {y}")
                cv2.setMouseCallback('Select Object', lambda *args: None)

        cv2.setMouseCallback('Select Object', click_handler)
        while True:
            key = cv2.waitKey(100)
            if point:
                break
            if cv2.getWindowProperty('Select Object', cv2.WND_PROP_VISIBLE) < 1:
                print("âŒ çª—å£è¢«å…³é—­ï¼Œæœªè¿›è¡Œç‚¹å‡»")
                return None
        cv2.destroyAllWindows()
        results = predictor(points=[point], labels=[1])
        center, mask = process_sam_results(results)

    # 5. ä¿å­˜åˆ†å‰²æ©ç 
    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… åˆ†å‰²æ©ç å·²ä¿å­˜ï¼š{output_mask}")
        return mask
    else:
        print("âš ï¸ åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")
        # è¿”å›é»‘è‰²çš„å…¨é›¶æ©ç ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
        h, w = image_input.shape[:2]
        return np.zeros((h, w), dtype=np.uint8)



# ----------------------- ä¸»ç¨‹åºå…¥å£ -----------------------
if __name__ == '__main__':
    seg_mask = segment_image('color_img_path.jpg')
    print("Segmentation result mask shape:", seg_mask.shape if seg_mask is not None else None)
