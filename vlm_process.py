import cv2
import numpy as np
import torch
from ultralytics.models.sam import Predictor as SAMPredictor

# import whisper
import json
import re
import base64
import textwrap
import queue
import time
import io
import os  # å¯¼å…¥ os ç”¨äºå¤„ç†ç¯å¢ƒå˜é‡

# å¼ºåˆ¶æ¸…é™¤å¯èƒ½å¯¼è‡´é”™è¯¯çš„ä»£ç†ç¯å¢ƒå˜é‡
for key in ['all_proxy', 'ALL_PROXY', 'http_proxy', 'HTTP_PROXY', 'https_proxy', 'HTTPS_PROXY']:
    os.environ.pop(key, None)

# import soundfile as sf  
# import sounddevice as sd
# from scipy.io.wavfile import write
# from pydub import AudioSegment

from openai import OpenAI  # å¯¼å…¥OpenAIå®¢æˆ·ç«¯
import httpx  # å¯¼å…¥ httpx ç”¨äºå¤„ç†ä»£ç†é—®é¢˜

import logging
# ç¦ç”¨ Ultralytics çš„æ—¥å¿—è¾“å‡º
logging.getLogger("ultralytics").setLevel(logging.WARNING)

from google import genai
from google.genai import types

# å¯¼å…¥å…¨å±€é…ç½®
from config import Config


# ----------------------- æŒ‡ä»¤è§£æä¸æ”¾ç½®ä½ç½®è¯†åˆ« -----------------------

def parse_instruction(user_input, image_input=None):
    """
    è§£æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œåˆ†ç¦»æŠ“å–ç›®æ ‡å’Œæ”¾ç½®ä½ç½®æè¿°ã€‚

    è¾“å…¥: "æŠŠåŸ¹å…»çš¿æ”¾ç½®åˆ°æ˜¾å¾®é•œçš„å³è¾¹çº¢è‰²åŒºåŸŸ"
    è¾“å‡º: {
        "grasp_target": "åŸ¹å…»çš¿",
        "place_description": "æ˜¾å¾®é•œçš„å³è¾¹çº¢è‰²åŒºåŸŸ",
        "has_place_instruction": True
    }
    """
    client = OpenAI(
        api_key=Config.QWEN_API_KEY,
        base_url=Config.QWEN_BASE_URL,
        http_client=httpx.Client(trust_env=False)
    )

    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººæŒ‡ä»¤è§£æç³»ç»Ÿã€‚è¯·åˆ†æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

    1. è¦æŠ“å–çš„ç‰©ä½“åç§°ï¼ˆgrasp_targetï¼‰
    2. æ”¾ç½®ä½ç½®çš„æè¿°ï¼ˆplace_descriptionï¼‰- å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ”¾ç½®ä½ç½®ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²

    ã€ç¤ºä¾‹ã€‘
    è¾“å…¥: "æŠŠåŸ¹å…»çš¿æ”¾ç½®åˆ°æ˜¾å¾®é•œçš„å³è¾¹"
    è¾“å‡º: {"grasp_target": "åŸ¹å…»çš¿", "place_description": "æ˜¾å¾®é•œçš„å³è¾¹", "has_place_instruction": true}

    è¾“å…¥: "æŠ“å–çº¢è‰²çš„è¯•ç®¡"
    è¾“å‡º: {"grasp_target": "çº¢è‰²çš„è¯•ç®¡", "place_description": "", "has_place_instruction": false}

    è¾“å…¥: "æŠŠçƒ§æ¯ç§»åˆ°æ¡Œå­å·¦ä¸Šè§’çš„çº¢è‰²åŒºåŸŸ"
    è¾“å‡º: {"grasp_target": "çƒ§æ¯", "place_description": "æ¡Œå­å·¦ä¸Šè§’çš„çº¢è‰²åŒºåŸŸ", "has_place_instruction": true}

    ã€æ³¨æ„ã€‘
    - åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
    - å¦‚æœæŒ‡ä»¤ä¸­åŒ…å«"æ”¾åˆ°"ã€"æ”¾ç½®åˆ°"ã€"ç§»åˆ°"ã€"ç§»åŠ¨åˆ°"ç­‰è¯è¯­ï¼Œè¯´æ˜æœ‰æ”¾ç½®æŒ‡ä»¤
    """)

    messages = [{"role": "system", "content": system_prompt}]
    user_content = [{"type": "text", "text": f"ç”¨æˆ·æŒ‡ä»¤ï¼š{user_input}"}]

    # å¦‚æœæä¾›äº†å›¾åƒï¼Œä¹Ÿå¯ä»¥å¸®åŠ©ç†è§£ä¸Šä¸‹æ–‡
    if image_input is not None:
        base64_img = encode_np_array(image_input)
        user_content.insert(0, {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}
        })

    messages.append({"role": "user", "content": user_content})

    try:
        completion = client.chat.completions.create(
            model=Config.QWEN_MODEL,
            messages=messages,
            temperature=Config.DEFAULT_TEMPERATURE,
        )

        content = completion.choices[0].message.content
        print(f"[æŒ‡ä»¤è§£æ] åŸå§‹å“åº”: {content}")

        # è§£æJSON
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            result = json.loads(match.group(1))
            return result
        else:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå‡è®¾æ•´ä¸ªè¾“å…¥éƒ½æ˜¯æŠ“å–ç›®æ ‡
            return {
                "grasp_target": user_input,
                "place_description": "",
                "has_place_instruction": False
            }

    except Exception as e:
        print(f"[æŒ‡ä»¤è§£æ] å¤±è´¥: {e}")
        return {
            "grasp_target": user_input,
            "place_description": "",
            "has_place_instruction": False
        }


def detect_place_position(place_description, global_image, depth_image=None, extra_images=None):
    """
    ä½¿ç”¨VLMåœ¨å…¨å±€å›¾åƒä¸­è¯†åˆ«æ”¾ç½®ä½ç½®ã€‚
    æ”¯æŒå¤šç›¸æœºå›¾åƒè¾“å…¥ï¼Œæä¾›æ›´å…¨é¢çš„åœºæ™¯ç†è§£ã€‚

    å‚æ•°:
        place_description: æ”¾ç½®ä½ç½®æè¿°
        global_image: ä¸»ç›¸æœºå›¾åƒ (ç”¨äºåæ ‡è®¡ç®—)
        depth_image: æ·±åº¦å›¾åƒ (å¯é€‰)
        extra_images: é¢å¤–çš„ç›¸æœºå›¾åƒåˆ—è¡¨ (å¯é€‰ï¼Œç”¨äºè¾…åŠ©è¯†åˆ«)
    """
    client = OpenAI(
        api_key=Config.QWEN_API_KEY,
        base_url=Config.QWEN_BASE_URL,
        http_client=httpx.Client(trust_env=False)
    )

    h, w = global_image.shape[:2]

    # å¦‚æœæœ‰å¤šä¸ªç›¸æœºå›¾åƒï¼Œæ‹¼æ¥æˆä¸€å¼ å¤§å›¾ç”¨äºè¯†åˆ«
    if extra_images and len(extra_images) > 0:
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] ä½¿ç”¨å¤šç›¸æœºèåˆæ¨¡å¼ ({1 + len(extra_images)} ä¸ªè§†è§’)")
        # åˆ›å»ºæ‹¼æ¥å›¾åƒç”¨äºVLMè¯†åˆ«
        all_images = [global_image] + extra_images
        # æ°´å¹³æ‹¼æ¥æ‰€æœ‰å›¾åƒ
        # å…ˆè°ƒæ•´æ‰€æœ‰å›¾åƒåˆ°ç›¸åŒé«˜åº¦
        target_h = min(img.shape[0] for img in all_images)
        resized_images = []
        for img in all_images:
            scale = target_h / img.shape[0]
            new_w = int(img.shape[1] * scale)
            resized = cv2.resize(img, (new_w, target_h))
            resized_images.append(resized)
        combined_image = np.hstack(resized_images)
        combined_h, combined_w = combined_image.shape[:2]
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] æ‹¼æ¥å›¾åƒå°ºå¯¸: {combined_w} x {combined_h}")
        # ä¿å­˜æ‹¼æ¥å›¾åƒç”¨äºè°ƒè¯•
        cv2.imwrite("debug_combined_views.jpg", combined_image)
    else:
        combined_image = global_image
        combined_h, combined_w = h, w

    # è§£ææ”¾ç½®æè¿°ï¼Œæå–å‚è€ƒç‰©ä½“å’Œæ–¹å‘
    reference_object = None
    direction = None
    color_region = None  # æ–°å¢ï¼šé¢œè‰²åŒºåŸŸ

    # æ£€æŸ¥æ˜¯å¦æ˜¯é¢œè‰²åŒºåŸŸæè¿°
    color_pattern = r'(çº¢è‰²|ç»¿è‰²|è“è‰²|é»„è‰²|ç™½è‰²|é»‘è‰²|æ©™è‰²|ç´«è‰²)(çš„)?(åŒºåŸŸ|åœ°æ–¹|ä½ç½®|éƒ¨åˆ†)'
    color_match = re.search(color_pattern, place_description)
    if color_match:
        color_region = color_match.group(1)
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] æ£€æµ‹åˆ°é¢œè‰²åŒºåŸŸæè¿°: {color_region}")

    patterns = [
        (r'(.+?)çš„(å·¦è¾¹|å³è¾¹|ä¸Šé¢|ä¸‹é¢|å‰é¢|åé¢|æ—è¾¹)', lambda m: (m.group(1), m.group(2))),
        (r'(å·¦è¾¹|å³è¾¹|ä¸Šé¢|ä¸‹é¢|å‰é¢|åé¢)çš„(.+)', lambda m: (m.group(2), m.group(1))),
    ]

    for pattern, extractor in patterns:
        match_result = re.search(pattern, place_description)
        if match_result:
            reference_object, direction = extractor(match_result)
            break

    # è§£æè·ç¦»æè¿°ï¼Œè®¡ç®—åç§»é‡
    def parse_distance_offset(description):
        """
        æ ¹æ®æè¿°ä¸­çš„è·ç¦»ä¿¡æ¯è®¡ç®—åƒç´ åç§»é‡ã€‚
        ç›¸æœºè§†è§’ä¸‹ï¼Œå¤§çº¦ 1cm â‰ˆ 8-12 åƒç´ ï¼ˆå–å†³äºæ·±åº¦ï¼‰
        """
        # æ£€æŸ¥æ˜ç¡®çš„å˜ç±³æ•°å€¼
        cm_match = re.search(r'(\d+)[-~åˆ°]?(\d*)(?:cm|å˜ç±³)', description)
        if cm_match:
            cm_min = int(cm_match.group(1))
            cm_max = int(cm_match.group(2)) if cm_match.group(2) else cm_min
            avg_cm = (cm_min + cm_max) / 2
            # å¤§çº¦ 10 åƒç´ /å˜ç±³
            offset = int(avg_cm * 10)
            print(f"[è·ç¦»è§£æ] æ£€æµ‹åˆ°è·ç¦»: {cm_min}-{cm_max}cm â†’ åç§» {offset} åƒç´ ")
            return max(20, min(200, offset))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…

        # æ£€æŸ¥ç›¸å¯¹è·ç¦»æè¿°
        close_keywords = ['ç´§æŒ¨', 'ç´§è´´', 'è´´ç€', 'æŒ¨ç€', 'å¾ˆè¿‘', 'è¿‘ä¸€ç‚¹', 'ä¸è¦å¤ªè¿œ', 'ä¸è¦ç¦».*å¤ªè¿œ', 'é è¿‘']
        medium_keywords = ['æ—è¾¹', 'è¾¹ä¸Š', 'é™„è¿‘']
        far_keywords = ['è¿œä¸€ç‚¹', 'è¿œäº›', 'ç¦»è¿œ', 'ç¨è¿œ']

        for keyword in close_keywords:
            if re.search(keyword, description):
                print(f"[è·ç¦»è§£æ] æ£€æµ‹åˆ°è¿‘è·ç¦»å…³é”®è¯: '{keyword}' â†’ åç§» 40 åƒç´ ")
                return 40

        for keyword in medium_keywords:
            if re.search(keyword, description):
                print(f"[è·ç¦»è§£æ] æ£€æµ‹åˆ°ä¸­ç­‰è·ç¦»å…³é”®è¯: '{keyword}' â†’ åç§» 70 åƒç´ ")
                return 70

        for keyword in far_keywords:
            if re.search(keyword, description):
                print(f"[è·ç¦»è§£æ] æ£€æµ‹åˆ°è¿œè·ç¦»å…³é”®è¯: '{keyword}' â†’ åç§» 120 åƒç´ ")
                return 120

        # é»˜è®¤åç§»
        print(f"[è·ç¦»è§£æ] æœªæ£€æµ‹åˆ°è·ç¦»æè¿°ï¼Œä½¿ç”¨é»˜è®¤åç§» 80 åƒç´ ")
        return 80

    # è®¡ç®—åç§»é‡
    pixel_offset = parse_distance_offset(place_description)

    print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] è§£æç»“æœ: å‚è€ƒç‰©ä½“='{reference_object}', æ–¹å‘='{direction}', é¢œè‰²åŒºåŸŸ='{color_region}', åç§»={pixel_offset}åƒç´ ")

    # ===== æ–°å¢ï¼šé¢œè‰²åŒºåŸŸè¯†åˆ« =====
    if color_region:
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] ä½¿ç”¨é¢œè‰²åŒºåŸŸè¯†åˆ«æ¨¡å¼...")

        # é¢œè‰²æ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
        color_map = {
            "çº¢è‰²": "red",
            "ç»¿è‰²": "green",
            "è“è‰²": "blue",
            "é»„è‰²": "yellow",
            "ç™½è‰²": "white",
            "é»‘è‰²": "black",
            "æ©™è‰²": "orange",
            "ç´«è‰²": "purple",
        }
        color_en = color_map.get(color_region, color_region)

        color_prompt = f"""è¯·åœ¨å›¾åƒä¸­æ‰¾åˆ° {color_region}/{color_en} é¢œè‰²çš„åŒºåŸŸï¼Œå¹¶è¿”å›è¯¥åŒºåŸŸçš„ä¸­å¿ƒç‚¹åæ ‡ã€‚

ã€é‡è¦æç¤ºã€‘
- ä»”ç»†è§‚å¯Ÿæ¡Œé¢/å·¥ä½œå°ä¸Šçš„é¢œè‰²æ ‡è®°åŒºåŸŸ
- {color_region}åŒºåŸŸé€šå¸¸æ˜¯æ¡Œé¢ä¸Šçš„å½©è‰²æ ‡è®°æˆ–è´´çº¸
- å›¾åƒå°ºå¯¸: {w} x {h} åƒç´ 
- åæ ‡ç³»: å·¦ä¸Šè§’(0,0)ï¼Œxå‘å³å¢å¤§ï¼Œyå‘ä¸‹å¢å¤§

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{"found": true, "center": [x, y], "reason": "æ‰¾åˆ°{color_region}åŒºåŸŸçš„åŸå› "}}
å¦‚æœæ‰¾ä¸åˆ°{color_region}åŒºåŸŸï¼Œè¿”å›ï¼š
{{"found": false, "reason": "æœªæ‰¾åˆ°çš„åŸå› "}}"""

        messages = [{"role": "system", "content": color_prompt}]
        base64_img = encode_np_array(global_image)
        messages.append({"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}},
            {"type": "text", "text": f"è¯·æ‰¾åˆ°å›¾åƒä¸­çš„{color_region}åŒºåŸŸ"}
        ]})

        try:
            completion = client.chat.completions.create(
                model=Config.QWEN_MODEL, messages=messages, temperature=Config.DEFAULT_TEMPERATURE)
            content = completion.choices[0].message.content
            print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] é¢œè‰²åŒºåŸŸè¯†åˆ«å“åº”: {content}")

            json_match = re.search(r'(\{.*\})', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
                if result.get("found") and "center" in result:
                    place_x, place_y = result["center"]
                    place_x = max(0, min(w-1, int(place_x)))
                    place_y = max(0, min(h-1, int(place_y)))

                    return {
                        "place_point": [place_x, place_y],
                        "confidence": 0.85,
                        "reason": f"æ‰¾åˆ°{color_region}åŒºåŸŸåœ¨({place_x},{place_y})"
                    }
        except Exception as e:
            print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] é¢œè‰²åŒºåŸŸè¯†åˆ«å¤±è´¥: {e}")

    # ä¸¤é˜¶æ®µè¯†åˆ«
    if reference_object and direction:
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] ç¬¬ä¸€é˜¶æ®µï¼šè¯†åˆ«å‚è€ƒç‰©ä½“ '{reference_object}'...")

        # ä¸ºå¸¸è§ç‰©ä½“æ·»åŠ è‹±æ–‡å’Œæè¿°æ€§æç¤º
        object_hints = {
            "æ˜¾å¾®é•œ": "æ˜¾å¾®é•œ/microscope (é»‘è‰²çš„å…‰å­¦è®¾å¤‡ï¼Œæœ‰ç›®é•œå’Œç‰©é•œï¼Œé€šå¸¸åœ¨æ¡Œé¢ä¸Š)",
            "æœºæ¢°è‡‚": "æœºæ¢°è‡‚/robot arm (é“¶è‰²æˆ–ç°è‰²çš„æœºæ¢°æ‰‹è‡‚)",
            "æ¡Œå­": "æ¡Œå­/table (å·¥ä½œå°é¢)",
        }
        search_hint = object_hints.get(reference_object, reference_object)

        # ä½¿ç”¨å¤šè§†è§’å›¾åƒï¼ˆå¦‚æœæœ‰ï¼‰
        if extra_images and len(extra_images) > 0:
            stage1_prompt = f"""è¿™æ˜¯ä»å¤šä¸ªè§’åº¦æ‹æ‘„çš„åœºæ™¯å›¾åƒï¼ˆæ°´å¹³æ‹¼æ¥ï¼‰ã€‚
è¯·åœ¨å›¾åƒä¸­æ‰¾åˆ° "{search_hint}" å¹¶è¿”å›å…¶åœ¨ã€ç¬¬ä¸€å¼ å›¾ï¼ˆæœ€å·¦è¾¹ï¼‰ã€‘ä¸­çš„ä½ç½®ã€‚

ã€é‡è¦æç¤ºã€‘
- å›¾åƒæ˜¯å¤šä¸ªè§†è§’çš„æ‹¼æ¥ï¼Œè¯·ç»¼åˆæ‰€æœ‰è§†è§’æ¥è¯†åˆ«ç‰©ä½“
- æ˜¾å¾®é•œé€šå¸¸æ˜¯é»‘è‰²çš„å…‰å­¦è®¾å¤‡ï¼Œæœ‰åœ†æŸ±å½¢çš„é•œç­’
- è¿”å›çš„åæ ‡å¿…é¡»æ˜¯åœ¨ç¬¬ä¸€å¼ å›¾ï¼ˆæœ€å·¦è¾¹ï¼Œå®½åº¦çº¦{w}åƒç´ ï¼‰ä¸­çš„ä½ç½®

ç¬¬ä¸€å¼ å›¾å°ºå¯¸: {w} x {h} åƒç´ ã€‚
åªè¿”å›JSONï¼š{{"found": true, "bbox": [x1,y1,x2,y2], "center": [cx,cy]}}
å¦‚æœç¡®å®æ‰¾ä¸åˆ°è¿”å›ï¼š{{"found": false}}"""
            image_for_vlm = combined_image
        else:
            stage1_prompt = f"""è¯·åœ¨å›¾åƒä¸­æ‰¾åˆ° "{search_hint}" å¹¶è¿”å›å…¶è¾¹ç•Œæ¡†å’Œä¸­å¿ƒç‚¹ã€‚

ã€é‡è¦æç¤ºã€‘
- ä»”ç»†è§‚å¯Ÿæ•´ä¸ªå›¾åƒ
- æ˜¾å¾®é•œé€šå¸¸æ˜¯é»‘è‰²çš„å…‰å­¦è®¾å¤‡ï¼Œæœ‰åœ†æŸ±å½¢çš„é•œç­’
- å¦‚æœçœ‹åˆ°ç±»ä¼¼çš„è®¾å¤‡ï¼Œè¯·æ ‡è®°å®ƒçš„ä½ç½®

å›¾åƒå°ºå¯¸: {w} x {h} åƒç´ ã€‚
åªè¿”å›JSONï¼š{{"found": true, "bbox": [x1,y1,x2,y2], "center": [cx,cy]}}
å¦‚æœç¡®å®æ‰¾ä¸åˆ°è¿”å›ï¼š{{"found": false}}"""
            image_for_vlm = global_image

        messages = [{"role": "system", "content": stage1_prompt}]
        base64_img = encode_np_array(image_for_vlm)
        messages.append({"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}},
            {"type": "text", "text": f"è¯·æ‰¾åˆ°å›¾åƒä¸­çš„ {search_hint}"}
        ]})

        try:
            completion = client.chat.completions.create(
                model=Config.QWEN_MODEL, messages=messages, temperature=Config.DEFAULT_TEMPERATURE)
            content = completion.choices[0].message.content
            print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] ç¬¬ä¸€é˜¶æ®µå“åº”: {content}")

            json_match = re.search(r'(\{.*\})', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(1))
                if result.get("found") and "center" in result:
                    ref_x, ref_y = result["center"]
                    print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] æ‰¾åˆ° '{reference_object}' åœ¨ ({ref_x}, {ref_y})")

                    # ä½¿ç”¨è§£æå‡ºçš„åç§»é‡
                    if direction == "å·¦è¾¹":
                        place_x, place_y = ref_x - pixel_offset, ref_y
                    elif direction == "å³è¾¹":
                        place_x, place_y = ref_x + pixel_offset, ref_y
                    elif direction in ["ä¸Šé¢", "å‰é¢"]:
                        place_x, place_y = ref_x, ref_y - pixel_offset
                    elif direction in ["ä¸‹é¢", "åé¢"]:
                        place_x, place_y = ref_x, ref_y + pixel_offset
                    else:
                        place_x, place_y = ref_x + pixel_offset, ref_y

                    place_x = max(0, min(w-1, int(place_x)))
                    place_y = max(0, min(h-1, int(place_y)))

                    return {
                        "place_point": [place_x, place_y],
                        "confidence": 0.9,
                        "reason": f"'{reference_object}'åœ¨({ref_x},{ref_y})ï¼Œ{direction}åç§»{pixel_offset}åƒç´ ",
                        "reference_position": [ref_x, ref_y]
                    }
        except Exception as e:
            print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] ç¬¬ä¸€é˜¶æ®µå¤±è´¥: {e}")

    # å›é€€åˆ°å•é˜¶æ®µè¯†åˆ«
    print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] ä½¿ç”¨å•é˜¶æ®µè¯†åˆ«...")
    system_prompt = f"""ä½ æ˜¯æœºå™¨äººè§†è§‰ç³»ç»Ÿã€‚ç”¨æˆ·æƒ³æŠŠç‰©ä½“æ”¾åˆ°ï¼š{place_description}

ã€å›¾åƒä¿¡æ¯ã€‘
- å›¾åƒå°ºå¯¸: {w} x {h} åƒç´ 
- åæ ‡ç³»: å·¦ä¸Šè§’(0,0)ï¼Œxå‘å³å¢å¤§ï¼Œyå‘ä¸‹å¢å¤§

ã€è¯†åˆ«æç¤ºã€‘
- å¦‚æœæè¿°ä¸­åŒ…å«é¢œè‰²ï¼ˆçº¢è‰²ã€ç»¿è‰²ã€è“è‰²ç­‰ï¼‰ï¼Œè¯·æ‰¾åˆ°æ¡Œé¢ä¸Šå¯¹åº”é¢œè‰²çš„æ ‡è®°åŒºåŸŸ
- å¦‚æœæè¿°ä¸­åŒ…å«å‚è€ƒç‰©ä½“ï¼ˆå¦‚æ˜¾å¾®é•œï¼‰ï¼Œè¯·å…ˆæ‰¾åˆ°è¯¥ç‰©ä½“ï¼Œå†ç¡®å®šç›¸å¯¹ä½ç½®
- æ˜¾å¾®é•œé€šå¸¸æ˜¯é»‘è‰²çš„å…‰å­¦è®¾å¤‡ï¼Œæœ‰åœ†æŸ±å½¢çš„é•œç­’
- æ¡Œé¢ä¸Šå¯èƒ½æœ‰å½©è‰²çš„æ ‡è®°åŒºåŸŸï¼ˆçº¢è‰²ã€ç»¿è‰²ç­‰ï¼‰

è¯·è¿”å›JSONæ ¼å¼ï¼š{{"place_point": [x, y], "confidence": 0.9, "reason": "åŸå› "}}"""

    messages = [{"role": "system", "content": system_prompt}]
    base64_img = encode_np_array(global_image)
    messages.append({"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}},
        {"type": "text", "text": f"æ”¾ç½®ä½ç½®ï¼š{place_description}"}
    ]})

    try:
        completion = client.chat.completions.create(
            model=Config.QWEN_MODEL, messages=messages, temperature=0.1)
        content = completion.choices[0].message.content
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] å“åº”: {content}")

        json_match = re.search(r'(\{.*\})', content, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group(1))
            if "place_point" in result:
                x, y = result["place_point"]
                result["place_point"] = [max(0,min(w-1,int(x))), max(0,min(h-1,int(y)))]
            return result
    except Exception as e:
        print(f"[æ”¾ç½®ä½ç½®è¯†åˆ«] å¤±è´¥: {e}")

    return None


def pixel_to_world(pixel_x, pixel_y, depth_img, T_wc, fovy, img_shape):
    """
    å°†åƒç´ åæ ‡è½¬æ¢ä¸ºä¸–ç•Œåæ ‡ã€‚

    å‚æ•°:
        pixel_x, pixel_y: åƒç´ åæ ‡
        depth_img: æ·±åº¦å›¾åƒ
        T_wc: ç›¸æœºåˆ°ä¸–ç•Œçš„å˜æ¢çŸ©é˜µ (spatialmath.SE3)
        fovy: å‚ç›´è§†åœºè§’ (å¼§åº¦)
        img_shape: å›¾åƒå°ºå¯¸ (height, width)

    è¿”å›:
        world_point: [x, y, z] ä¸–ç•Œåæ ‡
    """
    height, width = img_shape[:2]

    # è®¡ç®—ç›¸æœºå†…å‚
    focal = height / (2.0 * np.tan(fovy / 2.0))
    cx = width / 2.0
    cy = height / 2.0

    # è·å–æ·±åº¦å€¼
    depth = depth_img[int(pixel_y), int(pixel_x)]

    if depth <= 0 or np.isnan(depth) or np.isinf(depth):
        print(f"[è­¦å‘Š] æ·±åº¦å€¼æ— æ•ˆ: {depth}ï¼Œä½¿ç”¨é»˜è®¤æ¡Œé¢é«˜åº¦")
        # å‡è®¾æ¡Œé¢é«˜åº¦ä¸º0.74mï¼Œç›¸æœºé«˜åº¦çº¦3m
        depth = 2.5  # ä¼°è®¡æ·±åº¦

    # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»
    x_c = (pixel_x - cx) * depth / focal
    y_c = (pixel_y - cy) * depth / focal
    z_c = depth

    # è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
    point_camera = np.array([x_c, y_c, z_c, 1.0])
    point_world = T_wc.A @ point_camera

    return point_world[:3]


# ----------------------- åŸºç¡€å·¥å…·å‡½æ•° -----------------------

def encode_np_array(image_np):
    """å°† numpy å›¾åƒæ•°ç»„ï¼ˆBGRï¼‰ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    success, buffer = cv2.imencode('.jpg', image_np)
    if not success:
        raise ValueError("æ— æ³•å°†å›¾åƒæ•°ç»„ç¼–ç ä¸º JPEG")
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64



# ----------------------- å¤šæ¨¡æ€æ¨¡å‹è°ƒç”¨ï¼ˆQwenï¼‰ -----------------------

def generate_robot_actions(user_command, image_input=None):
    """
    ä½¿ç”¨ base64 çš„æ–¹å¼å°† numpy å›¾åƒå’Œç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤ä¼ ç»™ Qwen å¤šæ¨¡æ€æ¨¡å‹ï¼Œ
    è¦æ±‚æ¨¡å‹è¿”å›ä¸¤éƒ¨åˆ†ï¼š
      - æ¨¡å‹è¿”å›å†…å®¹ä¸­ï¼Œç¬¬ä¸€éƒ¨åˆ†ä¸ºè‡ªç„¶è¯­è¨€å“åº”ï¼ˆè¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“ï¼‰ï¼Œ
      - ç´§è·Ÿå…¶åçš„éƒ¨åˆ†ä¸ºçº¯ JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

        {
          "name": "ç‰©ä½“åç§°",
          "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
        }

    è¿”å›ä¸€ä¸ª dictï¼ŒåŒ…å« "response" å’Œ "coordinates"ã€‚
    å‚æ•° image_input ä¸º numpy æ•°ç»„ï¼ˆBGR æ ¼å¼ï¼‰ã€‚
    """
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œå½»åº•ç¦ç”¨ç¯å¢ƒä»£ç† (trust_env=False)
    # æ›¿æ¢ä¸ºè‡ªå·±çš„æ¨¡å‹è°ƒç”¨ï¼Œæ²¡æœ‰æœ¬åœ°éƒ¨ç½²çš„ï¼Œå¯ä»¥å‚è€ƒè¯¥ç½‘ç«™ https://sg.uiuiapi.com/v1
    client = OpenAI(
        api_key=Config.QWEN_API_KEY,
        base_url=Config.QWEN_BASE_URL,
        http_client=httpx.Client(trust_env=False)
    )       
    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ï¼Œå¹¶è®°å½•æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’ç‚¹å’Œå³ä¸‹è§’ç‚¹ï¼‰åŠå…¶ç±»åˆ«åç§°ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**,ä½†æ˜¯ä¸è¦è¿”å›jsonæœ¬ä½“,æ ¼å¼å¦‚ä¸‹ï¼š

    {
      "name": "ç‰©ä½“åç§°",
      "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
    }

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - JSON å¿…é¡»ä»ä¸‹ä¸€è¡Œå¼€å§‹ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
    - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š,åŒ…æ‹¬ä¸èƒ½æœ‰è¾…åŠ©æ ‡è¯†ä¸ºjsonæ–‡æœ¬çš„å†…å®¹,ä¸è¦æœ‰json;
    - åæ ‡ bbox å¿…é¡»ä¸ºæ•´æ•°ï¼›
    - åªå…è®¸ä½¿ç”¨ "bbox" ä½œä¸ºåæ ‡æ ¼å¼ã€‚
    """)

    messages = [{"role": "system", "content": system_prompt}]
    user_content = []

    if image_input is not None:
        base64_img = encode_np_array(image_input)
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_img}"
            }
        })

    user_content.append({"type": "text", "text": user_command})
    messages.append({"role": "user", "content": user_content})

    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨API
        completion = client.chat.completions.create(
            model=Config.QWEN_MODEL, 
            # model="gpt-5.2-2025-12-11",  # æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¯·ç¡®è®¤æœåŠ¡æä¾›å•†æ”¯æŒçš„æ¨¡å‹å
            # qwen3-omni-flash"
            # model="qwen-vl-plus",
            # model="qwen-vl-max", 
            # model="gpt-5",
            # model="qwen2.5-vl-32b-instruct",
            messages=messages,
            # max_tokens=4096,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
            temperature=Config.DEFAULT_TEMPERATURE,   # é™ä½æ¸©åº¦ä»¥æé«˜è¾“å‡ºçš„ç¡®å®šæ€§ï¼Œå¯¹ç»“æ„åŒ–è¾“å‡ºæœ‰ç›Š
        )
        
        content = completion.choices[0].message.content
        print("åŸå§‹å“åº”ï¼š", content)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            json_str = match.group(1)
            try:
                coord = json.loads(json_str)
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                coord = {}
            natural_response = content[:match.start()].strip()
        else:
            natural_response = content.strip()
            coord = {}

        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": "å¤„ç†å¤±è´¥", "coordinates": {}}


def generate_robot_actions_gemini(user_command, image_input=None):
    """
    ä½¿ç”¨ Google Gemini Robotics-ER 1.5 æ¨¡å‹å¤„ç†å›¾åƒå’ŒæŒ‡ä»¤ã€‚
    ä½¿ç”¨ä¸åŸ Qwen/OpenAI ç›¸åŒçš„æç¤ºè¯é€»è¾‘ï¼Œä½†é€‚é… Gemini çš„è¾“å…¥è¾“å‡ºã€‚
    """
    # æ›¿æ¢ä¸ºç”¨æˆ·çš„ API Key
    client = genai.Client(api_key=Config.GEMINI_API_KEY)
    MODEL_ID = Config.GEMINI_MODEL

    if image_input is None:
        return {"response": "éœ€è¦å›¾åƒè¾“å…¥", "coordinates": {}}

    # å°† numpy BGR å›¾åƒè½¬ä¸º RGB å¹¶ç¼–ç ä¸º bytes
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)
    success, encoded_image = cv2.imencode('.jpg', image_rgb)
    if not success:
         return {"response": "å›¾åƒç¼–ç å¤±è´¥", "coordinates": {}}
    image_bytes = encoded_image.tobytes()

    # å¤ç”¨åŸæœ‰çš„ System Prompt é€»è¾‘ï¼Œä¿æŒå®éªŒä¸€è‡´æ€§
    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ ({user_command})ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**, æ ¼å¼å¦‚ä¸‹ï¼š

    [
      {{
        "box_2d": [ymin, xmin, ymax, xmax],
        "label": "ç‰©ä½“åç§°"
      }}
    ]

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - åæ ‡ box_2d å¿…é¡»ä¸º 0-1000 çš„å½’ä¸€åŒ–æ•´æ•° (Gemini æ ‡å‡†)ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
    - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€‚
    """)

    # ç»„åˆ Prompt
    full_prompt = system_prompt.format(user_command=user_command)

    try:
        response = client.models.generate_content(
            model=MODEL_ID,
            contents=[
                types.Part.from_bytes(
                    data=image_bytes,
                    mime_type='image/jpeg',
                ),
                full_prompt
            ],
            config = types.GenerateContentConfig(
                temperature=0.5,
                thinking_config=types.ThinkingConfig(thinking_budget=1024) 
            )
        )
        
        content = response.text
        print("Gemini åŸå§‹å“åº”ï¼š", content)

        # è§£æå“åº”
        # å¯»æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\[.*\])', content, re.DOTALL)
        natural_response = content
        coord = {}

        if match:
            json_str = match.group(1)
            natural_response = content[:match.start()].strip()
            try:
                items = json.loads(json_str)
                if items and len(items) > 0:
                    item = items[0]
                    #å¤„ç†åæ ‡è½¬æ¢ï¼šGemini (0-1000) -> åƒç´ 
                    h, w = image_input.shape[:2]
                    box_2d = item.get("box_2d")
                    
                    if box_2d:
                        ymin, xmin, ymax, xmax = box_2d
                        x1 = int(xmin / 1000 * w)
                        y1 = int(ymin / 1000 * h)
                        x2 = int(xmax / 1000 * w)
                        y2 = int(ymax / 1000 * h)
                        
                        coord = {
                            "name": item.get("label", "target"),
                            "bbox": [x1, y1, x2, y2]
                        }
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                # å°è¯•ç¨å¾®æ¸…æ´—ä¸‹ json å†è§£æ
        
        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"Gemini è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": f"å¤„ç†å¤±è´¥: {e}", "coordinates": {}}


# ----------------------- SAM åˆ†å‰²ç›¸å…³ -----------------------
def choose_model():
    """Initialize SAM predictor with proper parameters"""
    model_weight = 'sam_b.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        # imgsz=1024,
        model=model_weight,
        conf=0.25,
        save=False
    )
    return SAMPredictor(overrides=overrides)

def process_sam_results(results):
    """Process SAM results to get mask and center point"""
    if not results or not results[0].masks:
        return None, None

    # Get first mask (assuming single object segmentation)
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255

    # Find contour and center
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None

    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask

    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


# ----------------------- è¯­éŸ³è¯†åˆ«ä¸ TTS -----------------------

# åˆå§‹åŒ–å…¨å±€æ¨¡å‹å˜é‡
_global_models = {}


def load_models():
    """åœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹ï¼Œé¿å…å¯åŠ¨æ—¶å…¨éƒ¨åŠ è½½å ç”¨èµ„æº"""
    if not _global_models:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç¦»çº¿è¯­éŸ³æ¨¡å‹...")
        # åŠ è½½Whisperå°å‹æ¨¡å‹ (é€‚åˆä½ çš„6GBæ˜¾å­˜)
        # _global_models['asr'] = whisper.load_model("small")
        # _global_models['asr'] = whisper.load_model("tiny")
        # _global_models['asr'] = whisper.load_model("base")
        print("âœ… Whisperçš„baseæ¨¡å‹åŠ è½½å®Œæ¯•")

        try:
            import pyttsx3
            _global_models['tts_backup'] = pyttsx3.init()
            # é…ç½®TTS
            _global_models['tts_backup'].setProperty('rate', 160)  # è¯­é€Ÿ
            voices = _global_models['tts_backup'].getProperty('voices')
            for voice in voices:
                if 'chinese' in voice.name.lower() or 'zh' in voice.id.lower():
                    _global_models['tts_backup'].setProperty('voice', voice.id)
                    break
            print("âœ… TTS (pyttsx3) åˆå§‹åŒ–å®Œæ¯•")
        except Exception as e:
            print(f"âš ï¸  TTSåˆå§‹åŒ–å¤±è´¥: {e}")
            _global_models['tts_backup'] = None

    return _global_models


# éŸ³é¢‘å‚æ•°é…ç½®
samplerate = 16000
channels = 1
dtype = 'int16'
frame_duration = 0.2
frame_samples = int(frame_duration * samplerate)
silence_threshold = 250
silence_max_duration = 2.0
q = queue.Queue()


def rms(audio_frame):
    samples = np.frombuffer(audio_frame, dtype=np.int16)
    if samples.size == 0:
        return 0
    mean_square = np.mean(samples.astype(np.float32) ** 2)
    if np.isnan(mean_square) or mean_square < 1e-5:
        return 0
    return np.sqrt(mean_square)

def callback(indata, frames, time_info, status):
    if status:
        print("âš ï¸ çŠ¶æ€è­¦å‘Šï¼š", status)
    q.put(bytes(indata))

def recognize_speech():
    """å½•éŸ³å¹¶è¿”å›éŸ³é¢‘æ•°æ®ï¼ˆnumpy æ•°ç»„ï¼‰"""
    print("ğŸ™ï¸ å¯åŠ¨å½•éŸ³ï¼Œè¯·è¯´è¯...")
    # print("ğŸ’¡ è°ƒè¯•ä¿¡æ¯ï¼šæ­£åœ¨ç›‘æµ‹å®æ—¶éŸ³é‡ï¼ˆRMSï¼‰ï¼Œè¯·è§‚å¯Ÿä¸è¯´è¯æ—¶çš„åŸºç¡€å™ªéŸ³å€¼")
    audio_buffer = []
    is_speaking = False
    last_voice_time = time.time()

    with sd.RawInputStream(samplerate=samplerate, blocksize=frame_samples,
                           dtype=dtype, channels=channels, callback=callback):
        while True:
            frame = q.get()
            volume = rms(frame)
            current_time = time.time()

            # print(f"å®æ—¶éŸ³é‡ï¼ˆRMSï¼‰: {volume}") 

            if volume > silence_threshold:
                if not is_speaking:
                    print("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
                    is_speaking = True
                    audio_buffer = []
                audio_np = np.frombuffer(frame, dtype=np.int16)
                audio_buffer.append(audio_np)
                last_voice_time = current_time
            elif is_speaking and (current_time - last_voice_time > silence_max_duration):
                print("ğŸ›‘ åœæ­¢å½•éŸ³ï¼Œå‡†å¤‡è¯†åˆ«...")
                full_audio = np.concatenate(audio_buffer, axis=0)
                return full_audio
            elif not is_speaking and (current_time - last_voice_time > 10.0):
                print("ğŸ›‘ è¶…æ—¶ï¼šæœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥")
                return np.array([], dtype=np.int16)

def speech_to_text_offline(audio_data):
    """
    ä½¿ç”¨ç¦»çº¿Whisperæ¨¡å‹å°†å½•éŸ³æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬
    """
    print("ğŸ“¡ æ­£åœ¨è¿›è¡Œç¦»çº¿è¯­éŸ³è¯†åˆ«...")
    models = load_models()
    asr_model = models['asr']

    # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    temp_wav = "temp_audio.wav"
    write(temp_wav, samplerate, audio_data.astype(np.int16))

    try:
        # ä½¿ç”¨Whisperè¿›è¡Œè¯†åˆ«ï¼ŒæŒ‡å®šè¯­è¨€ä¸ºä¸­æ–‡ä»¥æé«˜ç²¾åº¦å’Œé€Ÿåº¦
        result = asr_model.transcribe(temp_wav, language="zh", fp16=torch.cuda.is_available())
        return result["text"].strip()
    except Exception as e:
        print(f"âŒ ç¦»çº¿è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        return ""

def play_tts_offline(text):
    """
    ä½¿ç”¨ç¦»çº¿TTSæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶æ’­æ”¾
    """
    if not text:
        return
        
    print(f"ğŸ“¢ ç¦»çº¿TTSæ’­æ”¾: {text}")
    models = load_models()

    try:
        if models['tts_backup'] is not None:
            models['tts_backup'].say(text)
            models['tts_backup'].runAndWait()

    except Exception as e:
        print("âŒ æ— å¯ç”¨TTSå¼•æ“")


def voice_command_to_keyword():
    """
    è·å–è¯­éŸ³å‘½ä»¤å¹¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
    ç›´æ¥è¿”å›è¯†åˆ«çš„æ–‡æœ¬æŒ‡ä»¤ã€‚
    """
    audio_data = recognize_speech()
    text = speech_to_text_offline(audio_data) # æ”¹ä¸ºè°ƒç”¨ç¦»çº¿ASR
    if not text:
        print("âš ï¸ æ²¡æœ‰è¯†åˆ«åˆ°æ–‡æœ¬")
        return ""
    print("ğŸ“ è¯†åˆ«æ–‡æœ¬ï¼š", text)
    # play_tts_offline(f"å·²æ”¶åˆ°æŒ‡ä»¤: {text}") # æ”¹ä¸ºè°ƒç”¨ç¦»çº¿TTS
    return text


# ----------------------- ä¸»æµç¨‹ï¼šå›¾åƒåˆ†å‰² -----------------------
def segment_image(image_input, output_mask='mask1.png', command_text=None):
    # 1. ä½¿ç”¨æ–‡å­—è·å–ç›®æ ‡æŒ‡ä»¤
    if command_text is None:
        print("ğŸ“ è¯·é€šè¿‡æ–‡å­—æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
        command_text = input("è¯·è¾“å…¥: ").strip()
    
    if not command_text:
        print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
        # è¿”å›é»‘è‰²çš„å…¨é›¶æ©ç ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
        h, w = image_input.shape[:2]
        return np.zeros((h, w), dtype=np.uint8)
        
    print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # # 1. ä½¿ç”¨è¯­éŸ³è·å–ç›®æ ‡æŒ‡ä»¤
    # print("ğŸ™ï¸ è¯·é€šè¿‡è¯­éŸ³æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
    # command_text = voice_command_to_keyword()
    # if not command_text:
    #     print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
    #     return None
    # print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    # --- Prompt Enhancing: è‡ªåŠ¨è¡¥å……è§†è§‰æè¿°ä»¥æé«˜è¯†åˆ«ç‡ ---
    enhanced_command = command_text
    if "åŸ¹å…»çš¿" in command_text:
        enhanced_command = f"{command_text} (green cylinder, small container, cup)"
    
    print(f"[DEBUG] VLM å¢å¼ºæç¤ºè¯: {enhanced_command}")
    
    result = generate_robot_actions(enhanced_command, image_input)
    # åˆ‡æ¢ä¸º Gemini æ¨¡å‹
    # result = generate_robot_actions_gemini(command_text, image_input)
    natural_response = result["response"]
    detection_info = result["coordinates"]
    print("è‡ªç„¶è¯­è¨€å›åº”ï¼š", natural_response)
    print("æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯ï¼š", detection_info)

    # ä»…å¯¹æ¨¡å‹è¿”å›çš„è‡ªç„¶è¯­è¨€å›åº”æ’­æŠ¥
    # play_tts_offline(natural_response)
    
    bbox = detection_info.get("bbox") if detection_info and "bbox" in detection_info else None
    
    # 3. å‡†å¤‡å›¾åƒä¾› SAM ä½¿ç”¨ï¼ˆè½¬æ¢ä¸º RGBï¼‰
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)

    # 4. åˆå§‹åŒ– SAMï¼Œå¹¶è®¾ç½®å›¾åƒ
    predictor = choose_model()
    predictor.set_image(image_rgb)

    if bbox:
        results = predictor(bboxes=[bbox])
        center, mask = process_sam_results(results)
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡,bbox:{bbox}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¯·ç‚¹å‡»å›¾åƒé€‰æ‹©å¯¹è±¡")
        cv2.namedWindow('Select Object', cv2.WINDOW_NORMAL)
        cv2.imshow('Select Object', image_input)
        point = []

        def click_handler(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                point.extend([x, y])
                print(f"ğŸ–±ï¸ ç‚¹å‡»åæ ‡ï¼š{x}, {y}")
                cv2.setMouseCallback('Select Object', lambda *args: None)

        cv2.setMouseCallback('Select Object', click_handler)
        while True:
            key = cv2.waitKey(100)
            if point:
                break
            if cv2.getWindowProperty('Select Object', cv2.WND_PROP_VISIBLE) < 1:
                print("âŒ çª—å£è¢«å…³é—­ï¼Œæœªè¿›è¡Œç‚¹å‡»")
                return None
        cv2.destroyAllWindows()
        results = predictor(points=[point], labels=[1])
        center, mask = process_sam_results(results)

    # 5. ä¿å­˜åˆ†å‰²æ©ç 
    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… åˆ†å‰²æ©ç å·²ä¿å­˜ï¼š{output_mask}")
        return mask
    else:
        print("âš ï¸ åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")
        # è¿”å›é»‘è‰²çš„å…¨é›¶æ©ç ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
        h, w = image_input.shape[:2]
        return np.zeros((h, w), dtype=np.uint8)



# ----------------------- ä¸»ç¨‹åºå…¥å£ -----------------------
if __name__ == '__main__':
    seg_mask = segment_image('color_img_path.jpg')
    print("Segmentation result mask shape:", seg_mask.shape if seg_mask is not None else None)
